{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "from src.classes.ClassFractionalSin import FractionalSin\n",
    "from src.classes.ClassFractionalBiPotential import FractionalBiPotential\n",
    "from src.classes.ClassFractionalQuadSin import FractionalQuadSin\n",
    "from configs.RecursiveVPSDE.Markovian_fSin.recursive_Markovian_fSinWithPosition_T256_H05_tl_5data import get_config\n",
    "from configs.RecursiveVPSDE.Markovian_fQuadSin.recursive_Markovian_fQuadSinWithPosition_T256_H05_tl_5data import get_config\n",
    "#from configs.RecursiveVPSDE.Markovian_fBiPot.recursive_Markovian_fBiPotWithPosition_T256_H05_tl_5data import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'def FDA_SDE_kernel(bw, x):\\n    return norm.pdf(x / bw) / bw\\n\\ndef optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\\n    # Note we assume we want to estimate the drift function at the same points as the ones we observe\\n    assert (path_observations.shape == time_points.shape)\\n    assert (time_points.shape == (num_paths, num_time_points))\\n    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\\n    assert (np.allclose(time_points, time_points[[0], :]))\\n    eval_time_points = time_points[0, :]\\n    T_data = time_points.reshape(num_paths, num_time_points, 1, 1) - time_points\\n    assert np.all([np.allclose(T_data[:,:, i, j], time_points-time_points[i, j]) for i in range(num_time_points) for j in range(num_time_points)])\\n    # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\\n    assert (T_data.shape == (num_paths, num_time_points, num_paths, num_time_points))\\n    X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\\n    assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points, num_paths, num_time_points))\\n    X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\\n    Y = np.vstack([path_observations]*(num_paths*num_time_points))\\n    assert (Y.shape == (num_paths*num_time_points, num_paths, num_time_points))\\n    Y_flat = Y.flatten()\\n\\n    kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\\n    assert (kernel_weights.shape == (num_paths, num_time_points, num_paths, num_time_points))\\n    betas = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params.reshape(-1, 1)\\n    assert (betas.shape == (local_poly_smooth_order + 1, 1))\\n    return betas, eval_time_points\\n\\ndef per_t_optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\\n    # Note we assume we want to estimate the drift function at the same points as the ones we observe\\n    assert (path_observations.shape == time_points.shape)\\n    assert (time_points.shape == (num_paths, num_time_points))\\n    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\\n    assert (np.allclose(time_points, time_points[[0], :]))\\n    betas = np.zeros((num_time_points,local_poly_smooth_order + 1))\\n    eval_time_points = time_points[0, :]\\n    for tidx in range(num_time_points):\\n        t = eval_time_points[tidx]\\n        # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\\n        T_data = time_points - t\\n        assert (T_data.shape == (num_paths, num_time_points))\\n        X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\\n        assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points))\\n        X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\\n        Y_flat = path_observations.flatten()\\n        kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\\n        assert (kernel_weights.shape == (num_paths, num_time_points))\\n        betas[tidx, :] = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params\\n    assert (betas.shape == (num_time_points, local_poly_smooth_order + 1))\\n    mean_hat_t = betas[:,0]\\n    assert (mean_hat_t.shape == (num_time_points, ))\\n    delta_mean_hat_t = betas[:, 1] / bandwidth\\n    assert (delta_mean_hat_t.shape == (num_time_points, ))\\n    # 2.3: For now, we can assume the diffusion function is known for convenience (and possibly easier implementation)\\n    # true_cov = np.nan # TODO\\n    # cov_hat_t = true_cov\\n    return mean_hat_t,delta_mean_hat_t, eval_time_points\\n\\ndef estimate_drift_from_iid_paths(eval_time_points, mean_hat_t, delta_mean_hat_t, num_time_points):\\n    # Step 3: Plug-in estimators\\n    # 3.1: From Step 2, we can consistently recover the mean and covariance functions of the latent process together with their derivatives\\n    # 3.2: We plug these into Eq (3.5) i.e., Eq (3.14) to transform global information to local information in the form of drift and diffusion functions\\n    assert (eval_time_points.shape[0] == num_time_points)\\n    drift_hats = np.zeros((num_time_points, ))\\n    for tidx in range(num_time_points):\\n        drift_hat_t = 0. if mean_hat_t[tidx] == 0. else delta_mean_hat_t[tidx] * 1./mean_hat_t[tidx]\\n        drift_hats[tidx] = drift_hat_t\\n    return drift_hats\\n\\n# Drift estimation function\\ndef cv_nadaraya_watson_estimate_drift(x0, path, delta_X, c_const):\\n    h = c_const*(path.shape[0] ** (-0.2))\\n    path_incs = np.diff(path)\\n    weights = gaussian_kernel(x=(path[:-1] - x0), bw=h)\\n    numerator = np.sum(weights * path_incs)\\n    denominator = np.sum(weights)\\n    return numerator / (delta_X * denominator) if denominator != 0 else 0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def FDA_SDE_kernel(bw, x):\n",
    "    return norm.pdf(x / bw) / bw\n",
    "\n",
    "def optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\n",
    "    # Note we assume we want to estimate the drift function at the same points as the ones we observe\n",
    "    assert (path_observations.shape == time_points.shape)\n",
    "    assert (time_points.shape == (num_paths, num_time_points))\n",
    "    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\n",
    "    assert (np.allclose(time_points, time_points[[0], :]))\n",
    "    eval_time_points = time_points[0, :]\n",
    "    T_data = time_points.reshape(num_paths, num_time_points, 1, 1) - time_points\n",
    "    assert np.all([np.allclose(T_data[:,:, i, j], time_points-time_points[i, j]) for i in range(num_time_points) for j in range(num_time_points)])\n",
    "    # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\n",
    "    assert (T_data.shape == (num_paths, num_time_points, num_paths, num_time_points))\n",
    "    X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\n",
    "    assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points, num_paths, num_time_points))\n",
    "    X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\n",
    "    Y = np.vstack([path_observations]*(num_paths*num_time_points))\n",
    "    assert (Y.shape == (num_paths*num_time_points, num_paths, num_time_points))\n",
    "    Y_flat = Y.flatten()\n",
    "\n",
    "    kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\n",
    "    assert (kernel_weights.shape == (num_paths, num_time_points, num_paths, num_time_points))\n",
    "    betas = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params.reshape(-1, 1)\n",
    "    assert (betas.shape == (local_poly_smooth_order + 1, 1))\n",
    "    return betas, eval_time_points\n",
    "\n",
    "def per_t_optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\n",
    "    # Note we assume we want to estimate the drift function at the same points as the ones we observe\n",
    "    assert (path_observations.shape == time_points.shape)\n",
    "    assert (time_points.shape == (num_paths, num_time_points))\n",
    "    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\n",
    "    assert (np.allclose(time_points, time_points[[0], :]))\n",
    "    betas = np.zeros((num_time_points,local_poly_smooth_order + 1))\n",
    "    eval_time_points = time_points[0, :]\n",
    "    for tidx in range(num_time_points):\n",
    "        t = eval_time_points[tidx]\n",
    "        # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\n",
    "        T_data = time_points - t\n",
    "        assert (T_data.shape == (num_paths, num_time_points))\n",
    "        X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\n",
    "        assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points))\n",
    "        X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\n",
    "        Y_flat = path_observations.flatten()\n",
    "        kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\n",
    "        assert (kernel_weights.shape == (num_paths, num_time_points))\n",
    "        betas[tidx, :] = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params\n",
    "    assert (betas.shape == (num_time_points, local_poly_smooth_order + 1))\n",
    "    mean_hat_t = betas[:,0]\n",
    "    assert (mean_hat_t.shape == (num_time_points, ))\n",
    "    delta_mean_hat_t = betas[:, 1] / bandwidth\n",
    "    assert (delta_mean_hat_t.shape == (num_time_points, ))\n",
    "    # 2.3: For now, we can assume the diffusion function is known for convenience (and possibly easier implementation)\n",
    "    # true_cov = np.nan # TODO\n",
    "    # cov_hat_t = true_cov\n",
    "    return mean_hat_t,delta_mean_hat_t, eval_time_points\n",
    "\n",
    "def estimate_drift_from_iid_paths(eval_time_points, mean_hat_t, delta_mean_hat_t, num_time_points):\n",
    "    # Step 3: Plug-in estimators\n",
    "    # 3.1: From Step 2, we can consistently recover the mean and covariance functions of the latent process together with their derivatives\n",
    "    # 3.2: We plug these into Eq (3.5) i.e., Eq (3.14) to transform global information to local information in the form of drift and diffusion functions\n",
    "    assert (eval_time_points.shape[0] == num_time_points)\n",
    "    drift_hats = np.zeros((num_time_points, ))\n",
    "    for tidx in range(num_time_points):\n",
    "        drift_hat_t = 0. if mean_hat_t[tidx] == 0. else delta_mean_hat_t[tidx] * 1./mean_hat_t[tidx]\n",
    "        drift_hats[tidx] = drift_hat_t\n",
    "    return drift_hats\n",
    "\n",
    "# Drift estimation function\n",
    "def cv_nadaraya_watson_estimate_drift(x0, path, delta_X, c_const):\n",
    "    h = c_const*(path.shape[0] ** (-0.2))\n",
    "    path_incs = np.diff(path)\n",
    "    weights = gaussian_kernel(x=(path[:-1] - x0), bw=h)\n",
    "    numerator = np.sum(weights * path_incs)\n",
    "    denominator = np.sum(weights)\n",
    "    return numerator / (delta_X * denominator) if denominator != 0 else 0\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def gaussian_kernel(bw, x):\n",
    "    return norm.pdf(x / bw) / bw\n",
    "\n",
    "def rmse_ignore_nans(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)  # Ignore NaNs in both arrays\n",
    "    return np.sqrt(np.mean((y_true[mask] - y_pred[mask]) ** 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "num_paths = 10152\n",
    "t0 = 0.\n",
    "ts_length = 256\n",
    "deltaT = 1./256\n",
    "t1 = deltaT*ts_length\n",
    "# Drift parameters\n",
    "isUnitInterval = True\n",
    "diff = 1.\n",
    "initial_state = 0.\n",
    "rvs = None\n",
    "H = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if \"QuadSin\" in config.data_path:\n",
    "    fQuadSin = FractionalQuadSin(quad_coeff=config.quad_coeff, sin_coeff= config.sin_coeff, sin_space_scale=config.sin_space_scale, diff=diff, X0=initial_state)\n",
    "    is_path_observations = np.array(\n",
    "        [fQuadSin.euler_simulation(H=H, N=ts_length, deltaT=deltaT, isUnitInterval=isUnitInterval, X0=initial_state, Ms=None, gaussRvs=rvs,\n",
    "                               t0=t0, t1=t1) for _ in (range(num_paths*10))]).reshape(\n",
    "        (num_paths*10, ts_length+1))\n",
    "elif \"fSin\" in config.data_path:\n",
    "    fSin = FractionalSin(mean_rev=config.mean_rev, space_scale=1, diff=diff, X0=initial_state)\n",
    "    is_path_observations = np.array(\n",
    "        [fSin.euler_simulation(H=H, N=ts_length, deltaT=deltaT, isUnitInterval=isUnitInterval, X0=initial_state, Ms=None, gaussRvs=rvs,\n",
    "                               t0=t0, t1=t1) for _ in (range(num_paths*10))]).reshape(\n",
    "        (num_paths*10, ts_length+1))\n",
    "elif \"fBiPot\" in config.data_path:\n",
    "    fBiPot = FractionalBiPotential(const=config.const, quartic_coeff=config.quartic_coeff, quad_coeff = config.quad_coeff, diff=diff, X0=initial_state)\n",
    "    is_path_observations = np.array(\n",
    "        [fBiPot.euler_simulation(H=H, N=ts_length, deltaT=deltaT, isUnitInterval=isUnitInterval, X0=initial_state, Ms=None, gaussRvs=rvs,\n",
    "                               t0=t0, t1=t1) for _ in (range(num_paths*10))]).reshape(\n",
    "        (num_paths*10, ts_length+1))\n",
    "\n",
    "is_idxs = np.arange(is_path_observations.shape[0])\n",
    "path_observations = is_path_observations[np.random.choice(is_idxs, size=num_paths, replace=False),:]\n",
    "# We note that we DO NOT evaluate the drift at time t_{0}=0\n",
    "# We therefore remove the first element of path_observations since it includes X_{t_{0}} = X_{0}\n",
    "# We also remove the last element since we never evaluate the drift at that point\n",
    "t0 = deltaT\n",
    "prevPath_observations = path_observations[:,1:-1]\n",
    "# We compute the path incs with respect to the prevPath_observations (since X_{t_{0}} != X_{0})\n",
    "path_incs = np.diff(path_observations, axis=1)[:, 1:]\n",
    "assert (prevPath_observations.shape == path_incs.shape)\n",
    "assert (path_incs.shape[1] == ts_length - 1)\n",
    "assert (path_observations.shape[1] == prevPath_observations.shape[1] + 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def IID_NW_estimator(prevPath_observations, path_incs, bw, x, t1, t0, truncate):\n",
    "    N, n = prevPath_observations.shape\n",
    "    kernel_weights_unnorm = gaussian_kernel(bw=bw, x=prevPath_observations[:, :, np.newaxis] - x)\n",
    "    denominator = np.sum(kernel_weights_unnorm, axis=(1,0)) / (N*n)\n",
    "    numerator = np.sum(kernel_weights_unnorm * path_incs[:, :, np.newaxis], axis=(1,0)) / N*(t1 - t0)\n",
    "    estimator = numerator/denominator\n",
    "    # This is the \"truncated\" discrete drift estimator to ensure appropriate risk bounds\n",
    "    if truncate:\n",
    "        m = np.min(denominator)\n",
    "        estimator[denominator <= m/2.] = 0.\n",
    "    return estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "assert (prevPath_observations.shape[1]*deltaT == (t1-t0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2032700902169775\n"
     ]
    }
   ],
   "source": [
    "# Note that because b(x) = sin(x) is bounded, we take \\epsilon = 0 hence we have following h_max\n",
    "eps = 0.\n",
    "log_h_min = np.log10(np.power(float(ts_length - 1), -(1./(2.-eps))))\n",
    "print(log_h_min)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def compute_cv_for_bw_per_path(i, _bw):\n",
    "    N = prevPath_observations.shape[0]\n",
    "    mask = np.arange(N) != i # Leave-one-out !\n",
    "    estimator = IID_NW_estimator(\n",
    "        prevPath_observations=prevPath_observations[mask, :],\n",
    "        path_incs=path_incs[mask, :],\n",
    "        bw=_bw,\n",
    "        x=prevPath_observations[i, :],\n",
    "        t1=t1,\n",
    "        t0=t0,\n",
    "        truncate = False\n",
    "    )\n",
    "    residual = estimator**2 * deltaT - 2 * estimator * path_incs[i, :]\n",
    "    cv = np.sum(residual)\n",
    "    if np.isnan(cv):\n",
    "        return np.inf\n",
    "    return cv\n",
    "\n",
    "def compute_cv_for_bw(_bw):\n",
    "    N = prevPath_observations.shape[0]\n",
    "    #cvs = Parallel(n_jobs=4)(delayed(compute_cv_for_bw_per_path)(i, _bw) for i in (range(N)))\n",
    "    cvs = [compute_cv_for_bw_per_path(i, _bw) for i in range(N)]\n",
    "    return np.sum(cvs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'import time\\nbws = np.logspace(-2, -0.05, 20)\\nCVs = np.zeros(len(bws))\\nfor h in tqdm(range(bws.shape[0])):\\n    CVs[h] = compute_cv_for_bw(bws[h])\\n    time.sleep(5)'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "bws = np.logspace(-2, -0.05, 20)\n",
    "CVs = np.zeros(len(bws))\n",
    "for h in tqdm(range(bws.shape[0])):\n",
    "    CVs[h] = compute_cv_for_bw(bws[h])\n",
    "    time.sleep(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "bw = bws[np.argmin(CVs)]\n",
    "print(CVs)\n",
    "bw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "numXs = 256\n",
    "if \"fQuadSin\" in config.data_path:\n",
    "    minx = -1.7\n",
    "elif \"fBiPot\" in config.data_path:\n",
    "    minx = -2\n",
    "elif \"fSin\" in config.data_path:\n",
    "    minx = -3\n",
    "maxx = -minx\n",
    "Xs = np.linspace(minx, maxx, numXs)\n",
    "num_dhats = 1\n",
    "\"\"\"fSin = FractionalSin(mean_rev=mean_rev, diff=diff, X0=initial_state, space_scale=space_scale)\n",
    "oos_path_observations = np.array(\n",
    "    [fSin.euler_simulation(H=H, N=ts_length, deltaT=deltaT, isUnitInterval=isUnitInterval, X0=initial_state, Ms=None, gaussRvs=rvs,\n",
    "                           t0=t0, t1=t1) for _ in (range(num_paths*10))]).reshape(\n",
    "    (num_paths*10, ts_length+1))\n",
    "idxs = np.arange(oos_path_observations.shape[0])\n",
    "unif_oos_drift_hats = np.zeros((numXs, num_dhats))\"\"\"\n",
    "\n",
    "unif_is_drift_hats = np.zeros((numXs, num_dhats))\n",
    "\n",
    "for k in tqdm(range(num_dhats)):\n",
    "    \"\"\"oos_ss_path_observations = oos_path_observations[np.random.choice(idxs, size=num_paths, replace=False),:]\n",
    "    oos_prevPath_observations = oos_ss_path_observations[:,1:-1]\n",
    "    oos_path_incs = np.diff(oos_ss_path_observations, axis=1)[:, 1:]\n",
    "    unif_oos_drift_hats[:,k] = IID_NW_estimator(prevPath_observations=oos_prevPath_observations, bw=bw, x=Xs, path_incs=oos_path_incs, t1=t1, t0=t0, truncate=True)\"\"\"\n",
    "    is_ss_path_observations = is_path_observations#[np.random.choice(is_idxs, size=num_paths, replace=False),:]\n",
    "    is_prevPath_observations = is_ss_path_observations[:,1:-1]\n",
    "    is_path_incs = np.diff(is_ss_path_observations, axis=1)[:, 1:]\n",
    "    unif_is_drift_hats[:, k] = IID_NW_estimator(prevPath_observations=is_prevPath_observations, bw=bw, x=Xs, path_incs=is_path_incs, t1=t1, t0=t0, truncate=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"fig, ax = plt.subplots(figsize=(14, 9))\n",
    "true_drift = mean_rev*np.sin(space_scale*Xs.flatten())\n",
    "mean_drift_hats = np.mean(unif_is_drift_hats, axis=1)\n",
    "rmse = rmse_ignore_nans(mean_drift_hats, true_drift)\n",
    "std = 0*np.std(unif_is_drift_hats, axis=1)\n",
    "true_drift[Xs < -1.7] = np.nan\n",
    "true_drift[Xs > 0.1] = np.nan\n",
    "mean_drift_hats[Xs < -1.7] = np.nan\n",
    "mean_drift_hats[Xs > 0.1] = np.nan\n",
    "plt.scatter(Xs, true_drift, color=\"red\", label=\"True Drift\")\n",
    "plt.errorbar(Xs, mean_drift_hats,fmt=\"o\", yerr=2*std, label=\"Drift Estimator\")\n",
    "ax.set_title(rf\"IS Uniform RMSE {round(rmse, 4)} for IID Nadaraya Estimator for {num_paths} Paths\", fontsize=24)\n",
    "ax.tick_params(labelsize=14)\n",
    "plt.ylabel(\"Drift Value\", fontsize=18)\n",
    "plt.xlabel(r\"State $X$\", fontsize=18)\n",
    "ax.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(rmse)\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "if \"QuadSin\" in config.data_path:\n",
    "    true_drift = 2.*config.quad_coeff * Xs.flatten() + config.sin_coeff * config.sin_space_scale*np.sin(config.sin_space_scale*Xs.flatten())\n",
    "elif \"fSin\" in config.data_path:\n",
    "    true_drift = config.mean_rev*np.sin(Xs.flatten())\n",
    "elif \"BiPot\" in config.data_path:\n",
    "    true_drift = -(4.*config.quartic_coeff * np.power(Xs, 3) + 2.*config.quad_coeff * Xs + config.const).flatten()\n",
    "mean_drift_hats = np.mean(unif_is_drift_hats, axis=1)\n",
    "rmse = rmse_ignore_nans(mean_drift_hats, true_drift)\n",
    "std = np.std(unif_is_drift_hats, axis=1)\n",
    "plt.scatter(Xs, true_drift, color=\"red\", label=\"True Drift\")\n",
    "plt.errorbar(Xs, mean_drift_hats,fmt=\"o\", yerr=0*std, label=\"Estimated Drift\")\n",
    "ax.set_title(rf\"RMSE {round(rmse,4)} for IID Nadaraya Estimator\", fontsize=40)\n",
    "ax.tick_params(labelsize=38)\n",
    "plt.ylabel(\"Drift Value\", fontsize=38)\n",
    "plt.xlabel(r\"State $X$\", fontsize=38)\n",
    "ax.legend(fontsize=24)\n",
    "plt.savefig(f\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModelPresentationImages/fQuadSin_Nadaraya.png\",  bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
