{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from configs import project_config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "from scipy.stats import mannwhitneyu, ks_2samp\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_drift(config, prev_paths):\n",
    "    if \"fOU\" in config.data_path:\n",
    "        try:\n",
    "            return -config.mean_rev * (prev_paths.numpy() - config.mean)\n",
    "        except AttributeError as e:\n",
    "            return -config.mean_rev * (prev_paths - config.mean)\n",
    "    elif \"fSin\" in config.data_path:\n",
    "        try:\n",
    "            return config.mean_rev * np.sin(prev_paths.numpy())\n",
    "        except AttributeError as e:\n",
    "            return config.mean_rev * np.sin(prev_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.RecursiveVPSDE.recursive_PostMeanScore_fOU_T256_H05_tl_5data import get_config as get_config_postmean\n",
    "config_postmean = get_config_postmean()\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "num_simulated_paths = 500\n",
    "data_shape = (num_simulated_paths, 1, 1)\n",
    "device = \"cpu\"\n",
    "\n",
    "real_time_scale = torch.linspace(start=1 / config_postmean.ts_length, end=1, steps=config_postmean.ts_length).to(device)\n",
    "\n",
    "max_diff_steps = config_postmean.max_diff_steps\n",
    "sample_eps = config_postmean.sample_eps\n",
    "mean_rev = config_postmean.mean_rev\n",
    "ts_step = 1 / config_postmean.ts_length\n",
    "eval_ts_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import ot\n",
    "def bootstrap_test_wasserstein(sample1, sample2, num_bootstrap=1000):\n",
    "    # Step 1: Compute the observed Wasserstein distance\n",
    "    M = ot.dist(sample1, sample2, metric='euclidean')\n",
    "    a = np.ones((sample1.shape[0],)) / sample1.shape[0]  # Uniform weights for X\n",
    "    b = np.ones((sample1.shape[0],)) / sample1.shape[0]  # Uniform weights for Y\n",
    "\n",
    "    #observed_distance = wasserstein_distance(sample1, sample2)\n",
    "    print(a.shape, b.shape, M.shape)\n",
    "    observed_distance = ot.lp.emd2(a, b, M = M)\n",
    "    # Step 2: Bootstrap samples\n",
    "    bootstrap_distances = []\n",
    "    combined = np.vstack([sample1, sample2])\n",
    "    for _ in range(num_bootstrap):\n",
    "        np.random.shuffle(combined)\n",
    "        obs_perm = combined[:sample1.shape[0], :]\n",
    "        sim_perm = combined[sample2.shape[0]:, :]\n",
    "\n",
    "        # Step 3: Compute Wasserstein distance for bootstrap samples\n",
    "        M = ot.dist(obs_perm, sim_perm, metric='euclidean')\n",
    "        boot_distance = (ot.lp.emd2(a, b, M = M))\n",
    "        #boot_distance = wasserstein_distance(boot_sample1, boot_sample2)\n",
    "        bootstrap_distances.append(boot_distance)\n",
    "\n",
    "    # Step 4: Calculate p-value and confidence intervals\n",
    "    bootstrap_distances = np.array(bootstrap_distances)\n",
    "    p_value = np.mean(bootstrap_distances >= observed_distance)\n",
    "    print(bootstrap_distances, observed_distance, p_value)\n",
    "\n",
    "    return observed_distance, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the generated paths and compare with exact paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the drift estimator for different sample paths across time\n",
    "for Nep in config_postmean.max_epochs:\n",
    "    save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "    try:\n",
    "        postMean_prevPaths = torch.load(save_path + \"_prevPaths\")\n",
    "        synthetic_paths = np.load(config_postmean.data_path)[np.random.choice(np.arange(config_postmean.dataSize), postMean_prevPaths.shape[0], replace=False),:]\n",
    "        print(config_postmean.data_path)\n",
    "        synthetic_paths = np.concatenate([np.zeros((synthetic_paths.shape[0], 1)), synthetic_paths[:,:-1]], axis=1)\n",
    "        synthetic_paths = synthetic_paths[:,:postMean_prevPaths.shape[1]]\n",
    "        print(bootstrap_test_wasserstein(postMean_prevPaths.numpy(), synthetic_paths)[1])\n",
    "        for pathid in range(num_simulated_paths):\n",
    "            plt.scatter(real_time_scale[:eval_ts_length].cpu(), postMean_prevPaths[pathid,:].cpu(), s=10)\n",
    "        plt.xlabel(\"Real Time\")\n",
    "        plt.ylabel(\"Path\")\n",
    "        plt.title(f\"Generated Sample Paths Epoch {Nep}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        for pathid in range(num_simulated_paths):\n",
    "            plt.scatter(real_time_scale[:eval_ts_length].cpu(), synthetic_paths[pathid,:], s=10)\n",
    "        plt.xlabel(\"Real Time\")\n",
    "        plt.ylabel(\"Path\")\n",
    "        plt.title(f\"True Paths\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        del postMean_prevPaths\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some marginal distributions for increments\n",
    "time_space = np.linspace((1. / config_postmean.ts_length), 1., num=config_postmean.ts_length)\n",
    "low = 0\n",
    "high = config_postmean.ts_length\n",
    "Nep = config_postmean.max_epochs[0]\n",
    "save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "postMean_prevPaths = torch.load(save_path + \"_prevPaths\").numpy()\n",
    "synthetic_paths = np.load(config_postmean.data_path)[np.random.choice(np.arange(config_postmean.dataSize), postMean_prevPaths.shape[0], replace=False),:]\n",
    "synthetic_paths = np.concatenate([np.zeros((synthetic_paths.shape[0], 1)), synthetic_paths[:,:-1]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot evolution of marginal density of increments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incs = np.diff(postMean_prevPaths, axis=1)[1:]\n",
    "synth_incs = np.diff(synthetic_paths, axis=1)[1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Initialize empty bars\n",
    "_ = ax.hist([], bins=30, color='blue', alpha=0.6, label='True', edgecolor='black')[2]\n",
    "_ = ax.hist([], bins=30, color='orange', alpha=0.6, label='Synthetic', edgecolor='black')[2]\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Increment Value', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_title(f\"Marginal Distributions for Increment\", fontsize=15)\n",
    "ax.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()  # Clear the previous histogram\n",
    "    # Plot true histogram\n",
    "    ax.hist(incs[:,frame], bins=50, color=\"blue\", density=True, alpha=0.5, label='Generated')\n",
    "    # Plot synthetic histogram\n",
    "    ax.hist(synth_incs[:, frame], bins=50, color=\"orange\",density=True, alpha=0.5, label='True')\n",
    "    print(ks_2samp(incs[:,frame], synth_incs[:, frame])[1], mannwhitneyu(incs[:,frame], synth_incs[:, frame])[1])\n",
    "    ax.set_xlabel('Increment Value', fontsize=15)\n",
    "    ax.set_ylabel('Density', fontsize=15)\n",
    "    ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.set_title(f\"Marginal Distributions for Increments at Time {frame + 1}\", fontsize=15)\n",
    "    ax.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(1, eval_ts_length, 50), interval=1000/1, repeat=False)\n",
    "\n",
    "plt.close(fig)\n",
    "display(HTML(ani.to_jshtml()))  # Converts animation to JavaScript HTML5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the marginal density of the path values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Initialize empty bars\n",
    "_ = ax.hist([], bins=30, color='blue', alpha=0.6, label='True', edgecolor='black')[2]\n",
    "_ = ax.hist([], bins=30, color='orange', alpha=0.6, label='Synthetic', edgecolor='black')[2]\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Path Value', fontsize=15)\n",
    "ax.set_ylabel('Density', fontsize=15)\n",
    "ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_title(f\"Marginal Distributions for Paths\", fontsize=15)\n",
    "ax.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()  # Clear the previous histogram\n",
    "    # Plot true histogram\n",
    "    ax.hist(postMean_prevPaths[:,frame], bins=50, color=\"blue\", density=True, alpha=0.5, label='Generated')\n",
    "    # Plot synthetic histogram\n",
    "    ax.hist(synthetic_paths[:, frame], bins=50, color=\"orange\",density=True, alpha=0.5, label='True')\n",
    "    print(ks_2samp(postMean_prevPaths[:,frame], synthetic_paths[:, frame])[1], mannwhitneyu(postMean_prevPaths[:,frame], synthetic_paths[:, frame])[1])\n",
    "    ax.set_xlabel('Path Value', fontsize=15)\n",
    "    ax.set_ylabel('Density', fontsize=15)\n",
    "    ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.set_title(f\"Marginal Distributions for Paths at Time {frame + 1}\", fontsize=15)\n",
    "    ax.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(1, eval_ts_length, 50), interval=1000/1, repeat=False)\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())  # Converts animation to JavaScript HTML5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if generated true drift agrees with what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Nep in config_postmean.max_epochs:\n",
    "    save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "    try:\n",
    "        print(save_path)\n",
    "        postMean_prevPaths = torch.load(save_path + \"_prevPaths\")\n",
    "        true_drift = torch.load(save_path + \"_driftTrue\")\n",
    "        for diffTime in range(0, config_postmean.max_diff_steps, 2500):\n",
    "            plt.scatter(postMean_prevPaths, get_true_drift(config_postmean, postMean_prevPaths), s=10, color=\"orange\",label=\"True\")\n",
    "            plt.scatter(postMean_prevPaths, true_drift[:,:,config_postmean.max_diff_steps-1-diffTime],alpha=0.05, s=10,color=\"blue\")\n",
    "            plt.title(f\"True Drift Check at Diff Time {config_postmean.max_diff_steps-1-diffTime}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    except FileNotFoundError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and store MSE for estimated drifts\n",
    "experiment_emses = {Nep: None for Nep in config_postmean.max_epochs}\n",
    "minbiasses = {Nep: None for Nep in config_postmean.max_epochs}\n",
    "for Nep in config_postmean.max_epochs:\n",
    "    save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "    try:\n",
    "        postMean_prevPaths = torch.load(save_path + \"_prevPaths\")\n",
    "        drift_est = torch.load(save_path + \"_driftEst\")\n",
    "        true_drift = config_postmean.mean_rev * torch.sin(postMean_prevPaths)\n",
    "        bias2 = torch.pow(true_drift.unsqueeze(-1) - drift_est,2).mean(axis=0)\n",
    "        #variance = torch.var(drift_est, axis=0)\n",
    "        del drift_est, postMean_prevPaths, true_drift\n",
    "        emses = bias2 #+ variance\n",
    "        experiment_emses[Nep] = emses\n",
    "        minbiasses[Nep] = np.argmin(emses, axis=1)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the drift estimates as a function of state (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "for Nep in config_postmean.max_epochs:\n",
    "    save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "    try:\n",
    "        postMean_prevPaths = torch.load(save_path + \"_prevPaths\").numpy()\n",
    "        drift_est = torch.load(save_path + \"_driftEst\").numpy()\n",
    "        true_drift = get_true_drift(config_postmean, postMean_prevPaths)\n",
    "        num_bins = 100\n",
    "        total_counts = None\n",
    "        combined_x = []\n",
    "        combined_y = []\n",
    "        # Accumulate data for all time steps\n",
    "        for t in range(1, eval_ts_length):\n",
    "            x_data = postMean_prevPaths[:, t]\n",
    "            y_data = drift_est[:, t, minbiasses[Nep][t]]\n",
    "\n",
    "            # Append data points for all time points\n",
    "            combined_x.extend(x_data)\n",
    "            combined_y.extend(y_data)\n",
    "\n",
    "        # Accumulate total counts from hexbin\n",
    "        hb = ax.hexbin(combined_x, combined_y, gridsize=num_bins, cmap='viridis', mincnt=1)\n",
    "        total_counts = hb.get_array()\n",
    "\n",
    "        # Set color limits based on the total counts\n",
    "        hb.set_clim(0, np.max(total_counts))\n",
    "\n",
    "        # Create the colorbar for the combined counts\n",
    "        cbar = plt.colorbar(hb, ax=ax)\n",
    "        cbar.ax.tick_params(labelsize=15)\n",
    "        cbar.set_label('Data Counts', fontsize=15)\n",
    "\n",
    "        # Plot the true drift scatter plot\n",
    "        plt.scatter(postMean_prevPaths, true_drift, s=10, alpha=0.5, color=\"orange\", label=\"True Drift\")\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.title(f\"Drift Estimates Against State at Training Epoch {Nep}\", fontsize=15)\n",
    "        plt.xlabel(\"$X_{t}$\", fontsize=15)\n",
    "        plt.ylabel(\"Drift Estimate\", fontsize=15)\n",
    "        ax.tick_params(axis='x', labelsize=15)\n",
    "        ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        del drift_est\n",
    "    except FileNotFoundError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the drift as a function of state for different SDE times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = config_postmean.max_epochs[0]\n",
    "save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "postMean_prevPaths = torch.load(save_path + \"_prevPaths\")\n",
    "drift_est = torch.load(save_path + \"_driftEst\")\n",
    "true_drift = get_true_drift(config_postmean, postMean_prevPaths)\n",
    "\n",
    "# Create initial hexbin plot to get the color mapping\n",
    "hb = ax.hexbin(postMean_prevPaths[:, 0], drift_est[:, 0, minbiasses[Nep][0]], mincnt=1, cmap='viridis')\n",
    "cbar = plt.colorbar(hb, ax=ax)  # Create colorbar based on initial hexbin plot\n",
    "cbar.ax.tick_params(labelsize=15)  # Set colorbar tick font size\n",
    "cbar.set_label('Data Counts', fontsize=15)   # Set colorbar label font size\n",
    "\n",
    "def update(frame):\n",
    "    ax.clear()  # Clear the previous histogram\n",
    "    hb = ax.hexbin(postMean_prevPaths[:, frame], drift_est[:,frame, minbiasses[Nep][frame]], mincnt=1)\n",
    "\n",
    "    cbar.mappable.set_array(hb.get_array())  # Update colorbar with the current hexbin data\n",
    "    fig.draw_without_rendering()\n",
    "    ax.scatter(postMean_prevPaths[:, frame], true_drift[:, frame], s=10, alpha=0.5, color=\"orange\", label=\"True Drift\")\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_title(f\"Drift Estimates Against State at Real Time {frame}\", fontsize=15)\n",
    "    ax.set_xlabel(\"$X_{t}$\", fontsize=15)\n",
    "    ax.set_ylabel(\"Drift Estimate\", fontsize=15)\n",
    "    ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(1, eval_ts_length, 30), interval=1000/1, repeat=False)\n",
    "plt.close(fig)\n",
    "display(HTML(ani.to_jshtml()))  # Converts animation to JavaScript HTML5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the density of the true drift values against the estimated drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Nep in config_postmean.max_epochs:\n",
    "    save_path = (project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(14, 9))\n",
    "        postMean_prevPaths = torch.load(save_path + \"_prevPaths\").numpy()\n",
    "        drift_est = torch.load(save_path + \"_driftEst\").numpy()\n",
    "        opt_ests = []\n",
    "        for t in range(eval_ts_length):\n",
    "            curr_drift_est = drift_est[:, t, minbiasses[Nep][t]]\n",
    "            assert(curr_drift_est.shape == (postMean_prevPaths.shape[0], ))\n",
    "            opt_ests.append(curr_drift_est)\n",
    "        plt.hist(np.concatenate(opt_ests).flatten(), bins=300, alpha=0.5, density=True, color=\"blue\", label=\"Estimated Drifts\")\n",
    "        true_drift = get_true_drift(config_postmean, postMean_prevPaths)\n",
    "        plt.hist(true_drift.flatten(), bins=300, alpha=0.5, density=True, color=\"orange\", label=\"True Drift\")\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_xlabel(\"Drift Values\", fontsize=15)\n",
    "        ax.set_ylabel(\"Density\", fontsize=15)\n",
    "        ax.set_title(\"Densities of True vs Optimally Estimated Drifts\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        del postMean_prevPaths, drift_est\n",
    "    except FileNotFoundError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = ((project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{12920}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\", \"\"))\n",
    "postMean_prevPaths = torch.load(save_path + \"_prevPaths\").numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the drift MSE across diffusion times and different real times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map for 256 points\n",
    "fig, ax = plt.subplots(figsize=(14,9))\n",
    "min_drifts = {Nep: [] for Nep in config_postmean.max_epochs}\n",
    "cmap = cm.viridis  # You can choose any color map\n",
    "norm = plt.Normalize(vmin=0, vmax=1)\n",
    "for Nepoch, values in experiment_emses.items():\n",
    "    for t in range(eval_ts_length):\n",
    "        plt.scatter(np.arange(0, config_postmean.max_diff_steps), values[t, :], s=10, color=cmap(t / (eval_ts_length - 1) ))\n",
    "        min_drifts[Nepoch].append(np.min(values[t, :].numpy()))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  # We have to set the array to empty for ScalarMappable\n",
    "    cbar = plt.colorbar(sm)\n",
    "    cbar.ax.tick_params(labelsize=15)  # Set colorbar tick font size\n",
    "    cbar.set_label('Real Time', fontsize=15)   # Set colorbar label font size\n",
    "    plt.title(f\"Drift MSE at Training Epoch {Nepoch}\", fontsize=15)\n",
    "    plt.xlabel(\"Diffusion Time\", fontsize=15)\n",
    "    plt.ylabel(\"Drift MSE\", fontsize=15)\n",
    "    ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the minimum drift MSE across real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and bias2 for the drift estimates across different diffusion times\n",
    "for Nepoch, values in min_drifts.items():\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(14, 9))\n",
    "        plt.scatter(np.linspace(0, 1., eval_ts_length), values, s=10, label=f\"Epoch {Nepoch}\")\n",
    "        plt.title(f\"Minimum MSE of Drift Estimator Across Real Times\", fontsize=15)\n",
    "        plt.xlabel(\"Real Time\", fontsize=15)\n",
    "        plt.ylabel(\"Minimum Drift MSE\", fontsize=15)\n",
    "        ax.tick_params(axis='x', labelsize=15)  # Set x-axis tick size to 12\n",
    "        ax.tick_params(axis='y', labelsize=15)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement non-parametric Bandi and Phillips Drift Estimator on a per-path basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def gaussian_kernel(u):\n",
    "    return norm.pdf(u)\n",
    "\n",
    "# Drift estimation function\n",
    "def nadaraya_watson_estimate_drift(x0, path, delta_X):\n",
    "    h = 5*(path.shape[0] ** (-0.2))\n",
    "    path_incs = np.diff(path)\n",
    "    weights = gaussian_kernel((path[:-1] - x0) / h)\n",
    "    numerator = np.sum(weights * path_incs)\n",
    "    denominator = np.sum(weights)\n",
    "    return numerator / (delta_X * denominator) if denominator != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise implementation on synthetic paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sde_learn.utils.data import (euler_maruyama, ornstein_uhlenbeck)\n",
    "# Generate synthetic paths.\n",
    "n = 1  # Dimension of the state variable\n",
    "mu = np.array([0.] * n)  # Mean\n",
    "theta = 0.8  # Rate of mean reversion\n",
    "sigma = 1#0.5 * theta ** (1 / 2)  # Volatility\n",
    "mean_0, std_0 = np.array([0.] * n), sigma / (2 * theta) ** (1 / 2)  # Mean and std at t=0\n",
    "T = 1.  # Ending time\n",
    "num_time_points = 256  # Number of time steps\n",
    "dt = T / num_time_points  # Time step\n",
    "num_paths = 1 # Number of paths to draw\n",
    "# Get drift and diffusion coefficients for the Ornstein–Uhlenbeck process.\n",
    "b, sigma_func = ornstein_uhlenbeck(mu=mu, theta=theta, sigma=sigma)\n",
    "\n",
    "# 2.2: Optimise for the \"local_poly_smooth_order\" + 1 dimensional vector for each time\n",
    "X_tr, Drifts_tr, _ = euler_maruyama(b, sigma_func, num_time_points, num_paths, T, n, mean_0, std_0, coef_save=True, time=False)\n",
    "X_tr = X_tr[:, :, 0]\n",
    "Drifts_tr = Drifts_tr[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mean_mse = 0.\n",
    "for pathid in range(X_tr.shape[0]):\n",
    "    kernel_drifts = []\n",
    "    path = X_tr[pathid, :]\n",
    "    td = Drifts_tr[pathid, :]\n",
    "    assert np.allclose(mu, np.zeros(mu.shape[0])) and np.allclose((td[1:] - (-theta*path[:-1])), np.zeros(td.shape[0]-1))\n",
    "    for pathvalid in range(path.shape[0]):\n",
    "        x0 = path[pathvalid]\n",
    "        kernel_drifts.append(nadaraya_watson_estimate_drift(x0, path, delta_X = dt))\n",
    "    kernel_drifts = np.array(kernel_drifts)\n",
    "    kernel_drift_mse = (np.mean(np.power(td-kernel_drifts,2)))\n",
    "    mean_mse += kernel_drift_mse\n",
    "    if pathid == 0:\n",
    "        fig, ax = plt.subplots(figsize=(14, 9))\n",
    "        plt.scatter(path[:-1], td[1:], label=\"True Drift\")\n",
    "        plt.scatter(path, kernel_drifts, label=f\"Nadaraya-Watson Estimate: MSE {round(kernel_drift_mse, 3)}\")\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.xlabel(\"$X_{t}$\", fontsize=15)\n",
    "        plt.ylabel(\"Drift Estimate\", fontsize=15)\n",
    "        plt.title(\"Nadaraya-Watson Drift Estimator\", fontsize=15)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "print(mean_mse / X_tr.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-version of simple kernel estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.vectors import FloatVector\n",
    "try:\n",
    "    # Import the sde package from R\n",
    "    sde = importr('sde')\n",
    "    # Create an R environment and import the ksmooth function\n",
    "    ts = pd.Series(path, index=T_tr.flatten())\n",
    "    # Convert Python lists to R FloatVectors\n",
    "    x_r = ro.r['ts'](ts)\n",
    "    # Use the ksmooth function from R's sde package (kernel smoothing)\n",
    "    ksmooth_result = sde.ksdrift(x_r, n=path.shape[0])\n",
    "    print(ksmooth_result)\n",
    "\n",
    "    # Extract the results\n",
    "    smoothed_x = list(ksmooth_result[0])  # Smoothed x-values\n",
    "    smoothed_y = list(ksmooth_result[1])  # Smoothed y-values\n",
    "\n",
    "    # Print the smoothed results\n",
    "    print(\"Smoothed x:\", smoothed_x)\n",
    "    print(\"Smoothed y:\", smoothed_y)\n",
    "except NameError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from \"Nonparametric estimation for SDE with sparsely sampled paths\" (Mohammadi, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Linking global and local (FDA vs SDE) parameterisations\n",
    "# 1.1: (3.4) and (3.5) to establish bijective correspondence between drift/diffusion and mean/covariance (local vs global)\n",
    "# 1.2: Offdiagonal elements of covariance matrices of observations are proxies for the second moment function of the latent process\n",
    "# 1.2: E[YijYik] = E[X(Tik)X(tik)] + delta_(j,k)*v^2, where Yi is the observed path corrupted by additive noise from the discretely sampled\n",
    "     # true SDE path, Xi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pooling data and estimating the covariance\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "def FDA_SDE_kernel(bw, x):\n",
    "    return norm.pdf(x / bw) / bw\n",
    "\n",
    "def optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\n",
    "    # Note we assume we want to estimate the drift function at the same points as the ones we observe\n",
    "    assert (path_observations.shape == time_points.shape)\n",
    "    assert (time_points.shape == (num_paths, num_time_points))\n",
    "    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\n",
    "    assert (np.allclose(time_points, time_points[[0], :]))\n",
    "    eval_time_points = time_points[0, :]\n",
    "    T_data = time_points.reshape(num_paths, num_time_points, 1, 1) - time_points\n",
    "    assert np.all([np.allclose(T_data[:,:, i, j], time_points-time_points[i, j]) for i in range(num_time_points) for j in range(num_time_points)])\n",
    "    # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\n",
    "    assert (T_data.shape == (num_paths, num_time_points, num_paths, num_time_points))\n",
    "    X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\n",
    "    assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points, num_paths, num_time_points))\n",
    "    X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\n",
    "    Y = np.vstack([path_observations]*(num_paths*num_time_points))\n",
    "    assert (Y.shape == (num_paths*num_time_points, num_paths, num_time_points))\n",
    "    Y_flat = Y.flatten()\n",
    "\n",
    "    kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\n",
    "    assert (kernel_weights.shape == (num_paths, num_time_points, num_paths, num_time_points))\n",
    "    betas = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params.reshape(-1, 1)\n",
    "    assert (betas.shape == (local_poly_smooth_order + 1, 1))\n",
    "    return betas, eval_time_points\n",
    "\n",
    "def per_t_optimise_for_betahat(path_observations, time_points, bandwidth, local_poly_smooth_order, num_paths, num_time_points):\n",
    "    # Note we assume we want to estimate the drift function at the same points as the ones we observe\n",
    "    assert (path_observations.shape == time_points.shape)\n",
    "    assert (time_points.shape == (num_paths, num_time_points))\n",
    "    # Note we assume all paths have the same number of observations and are defined on the same grid size (FOR NOW)\n",
    "    assert (np.allclose(time_points, time_points[[0], :]))\n",
    "    betas = np.zeros((num_time_points,local_poly_smooth_order + 1))\n",
    "    eval_time_points = time_points[0, :]\n",
    "    for tidx in range(num_time_points):\n",
    "        t = eval_time_points[tidx]\n",
    "        # Bethat is the OLS parameter vector from regression path_observations against polynomial expansion of time differences\n",
    "        T_data = time_points - t\n",
    "        assert (T_data.shape == (num_paths, num_time_points))\n",
    "        X_data = np.stack([np.power(T_data, p) for p in range(local_poly_smooth_order+1)])\n",
    "        assert (X_data.shape == (local_poly_smooth_order + 1, num_paths, num_time_points))\n",
    "        X_flat = X_data.reshape((X_data.shape[0], np.prod(X_data.shape[1:]))).T\n",
    "        Y_flat = path_observations.flatten()\n",
    "        kernel_weights = np.sqrt(FDA_SDE_kernel(bw=bandwidth, x=T_data))\n",
    "        assert (kernel_weights.shape == (num_paths, num_time_points))\n",
    "        betas[tidx, :] = sm.WLS(Y_flat, X_flat, weights=kernel_weights.flatten()).fit().params\n",
    "    assert (betas.shape == (num_time_points, local_poly_smooth_order + 1))\n",
    "    mean_hat_t = betas[:,0]\n",
    "    assert (mean_hat_t.shape == (num_time_points, ))\n",
    "    delta_mean_hat_t = betas[:, 1] / bandwidth\n",
    "    assert (delta_mean_hat_t.shape == (num_time_points, ))\n",
    "    # 2.3: For now, we can assume the diffusion function is known for convenience (and possibly easier implementation)\n",
    "    # true_cov = np.nan # TODO\n",
    "    # cov_hat_t = true_cov\n",
    "    return mean_hat_t,delta_mean_hat_t, eval_time_points\n",
    "\n",
    "def estimate_drift_from_iid_paths(eval_time_points, mean_hat_t, delta_mean_hat_t, num_time_points):\n",
    "    # Step 3: Plug-in estimators\n",
    "    # 3.1: From Step 2, we can consistently recover the mean and covariance functions of the latent process together with their derivatives\n",
    "    # 3.2: We plug these into Eq (3.5) i.e., Eq (3.14) to transform global information to local information in the form of drift and diffusion functions\n",
    "    assert (eval_time_points.shape[0] == num_time_points)\n",
    "    drift_hats = np.zeros((num_time_points, ))\n",
    "    for tidx in range(num_time_points):\n",
    "        drift_hat_t = 0. if mean_hat_t[tidx] == 0. else delta_mean_hat_t[tidx] * 1./mean_hat_t[tidx]\n",
    "        drift_hats[tidx] = drift_hat_t\n",
    "    return drift_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise implementation on sample paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sde_learn.utils.data import (euler_maruyama, ornstein_uhlenbeck)\n",
    "# Generate synthetic paths.\n",
    "n = 1  # Dimension of the state variable\n",
    "mu = np.array([0.] * n)  # Mean\n",
    "theta = 0.8  # Rate of mean reversion\n",
    "sigma = 1#0.5 * theta ** (1 / 2)  # Volatility\n",
    "mean_0, std_0 = np.array([10.] * n), sigma / (2 * theta) ** (1 / 2)  # Mean and std at t=0\n",
    "T = 1.  # Ending time\n",
    "num_time_points = 256  # Number of time steps\n",
    "dt = T / num_time_points  # Time step\n",
    "num_paths = 500 # Number of paths to draw\n",
    "# Get drift and diffusion coefficients for the Ornstein–Uhlenbeck process.\n",
    "b, sigma_func = ornstein_uhlenbeck(mu=mu, theta=theta, sigma=sigma)\n",
    "\n",
    "# 2.2: Optimise for the \"local_poly_smooth_order\" + 1 dimensional vector for each time\n",
    "X_tr, Drifts_tr, _ = euler_maruyama(b, sigma_func, num_time_points, num_paths, T, n, mean_0, std_0, coef_save=True, time=False)\n",
    "X_tr = X_tr[:, :, 0]\n",
    "Drifts_tr = Drifts_tr[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_poly_smooth_order = 30#int(np.ceil(2*s/(1.-2*s)))\n",
    "path_observations = X_tr[:, :]\n",
    "T_tr = (np.arange(0, num_time_points) * dt).reshape(-1, 1)\n",
    "time_points = np.vstack([T_tr.T for _ in range(num_paths)])\n",
    "assert (time_points.shape == path_observations.shape)\n",
    "\n",
    "bandwidth = 1 * (np.log(num_paths) / num_paths) ** (1./(2*(local_poly_smooth_order + 1)))\n",
    "print(bandwidth)\n",
    "mean_hat_t, delta_mean_hat_t, eval_time_points = per_t_optimise_for_betahat(path_observations=path_observations, time_points=time_points, bandwidth=bandwidth, local_poly_smooth_order=local_poly_smooth_order, num_paths=num_paths, num_time_points=num_time_points)\n",
    "\n",
    "# Step 3: Plug-in estimator\n",
    "drift_hats = estimate_drift_from_iid_paths(eval_time_points=eval_time_points, mean_hat_t=mean_hat_t, delta_mean_hat_t=delta_mean_hat_t, num_time_points=num_time_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "time_mse = round(np.mean(np.power(drift_hats - (-theta), 2)), 3)\n",
    "plt.scatter(eval_time_points, drift_hats, color=\"blue\", label=f\"Estimated $\\mu(t)$ with MSE: {time_mse}\")\n",
    "plt.scatter(eval_time_points, [-theta]*num_time_points, color=\"orange\", label=\"True $\\mu(t)$\")\n",
    "plt.title(\"IID Path Estimation of $\\mu(t)$\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylabel(\"Time-component of Drift\", fontsize=15)\n",
    "plt.xlabel(\"Real Time\", fontsize=15)\n",
    "ax.tick_params(labelsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathid in range(path_observations.shape[0]):\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    path = path_observations[pathid, :]\n",
    "    td = Drifts_tr[pathid, :]\n",
    "    mse = round(np.mean(np.power(td[1:] - (drift_hats[1:]*path[1:]),2)), 3)\n",
    "    plt.scatter(path[:-1], drift_hats[1:]*path[1:], color=\"blue\", label=f\"Estimated Drift with MSE: {mse}\")\n",
    "    plt.scatter(path[:-1], td[1:], color=\"orange\", label=\"True Drift\")\n",
    "    plt.title(\"IID Path Estimation of Drift $\\mu(t, X_{t}) = \\mu(t)X_{t}$\", fontsize=15)\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.ylabel(\"Drift\", fontsize=15)\n",
    "    plt.xlabel(\"$X_{t}$\", fontsize=15)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now compare estimators with the score-based estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = config_postmean.max_epochs[0]\n",
    "save_path = ((project_config.ROOT_DIR + f\"experiments/results/TSPM_ES15_DriftEvalExp_{Nep}Nep_{config_postmean.loss_factor}LFactor_{config_postmean.mean}Mean_{config_postmean.max_diff_steps}DiffSteps\").replace(\".\",\"\")).replace(\".\",\"\")\n",
    "postMean_prevPaths = torch.load(save_path + \"_prevPaths\").numpy()\n",
    "drift_est = torch.load(save_path + \"_driftEst\").numpy()\n",
    "true_drift = get_true_drift(config_postmean, postMean_prevPaths)\n",
    "minbiasses = (np.argmin(np.mean(np.power(true_drift[:, :, np.newaxis] - drift_est, 2), axis=0), axis=1))\n",
    "opt_drift_est = np.concatenate([drift_est[:, [tidx], minbiasses[tidx]] for tidx in range(drift_est.shape[1])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Nadaraya-Watson Estimator on score-generated paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathid in range(0, opt_drift_est.shape[0], 100):\n",
    "    path = postMean_prevPaths[pathid, :]\n",
    "    num_time_points = path.shape[0]\n",
    "    td = true_drift[pathid, :]\n",
    "    sb_drift_est = opt_drift_est[pathid, :]\n",
    "    kernel_drifts = []\n",
    "    for pathvalid in range(path.shape[0]):\n",
    "        x0 = path[pathvalid]\n",
    "        kernel_drifts.append(nadaraya_watson_estimate_drift(x0, path, delta_X = config_postmean.end_diff_time/config_postmean.ts_length))\n",
    "    kernel_drifts = np.array(kernel_drifts)\n",
    "    kernel_drift_mse = (np.mean(np.power(td-kernel_drifts,2)))\n",
    "    sb_d_mse = np.mean(np.power(td-sb_drift_est,2))\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    plt.scatter(path, td, color=\"orange\", label=\"True Drift\")\n",
    "    plt.scatter(path, kernel_drifts, color=\"green\", label=f\"ND Estimate: MSE {round(kernel_drift_mse, 3)}\")\n",
    "    plt.scatter(path, sb_drift_est, color=\"blue\", label=f\"Score-Based Drift Estimator: MSE {round(sb_d_mse, 3)}\")\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(\"$X_{t}$\", fontsize=15)\n",
    "    plt.ylabel(\"Drift Estimate\", fontsize=15)\n",
    "    plt.title(\"Comparison of Drift Estimators\", fontsize=15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use IID Path Estimator on score-generated paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_poly_smooth_order = 30#int(np.ceil(2*s/(1.-2*s)))\n",
    "num_time_points = postMean_prevPaths.shape[1]\n",
    "num_paths = postMean_prevPaths.shape[0]\n",
    "dt = config_postmean.end_diff_time/config_postmean.ts_length\n",
    "\n",
    "path_observations = postMean_prevPaths\n",
    "T_tr = (np.arange(0, num_time_points) * dt).reshape(-1, 1)\n",
    "time_points = np.vstack([T_tr.T for _ in range(num_paths)])\n",
    "bandwidth = 1 * (np.log(num_paths) / num_paths) ** (1./(2*(local_poly_smooth_order + 1)))\n",
    "print(bandwidth)\n",
    "mean_hat_t, delta_mean_hat_t, eval_time_points = per_t_optimise_for_betahat(path_observations=path_observations, time_points=time_points, bandwidth=bandwidth, local_poly_smooth_order=local_poly_smooth_order, num_paths=num_paths, num_time_points=num_time_points)\n",
    "drift_hats = estimate_drift_from_iid_paths(eval_time_points=eval_time_points, mean_hat_t=mean_hat_t, delta_mean_hat_t=delta_mean_hat_t, num_time_points=num_time_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "time_mse = round(np.mean(np.power(drift_hats - (-config_postmean.mean), 2)), 3)\n",
    "plt.scatter(eval_time_points, drift_hats, color=\"blue\", label=f\"Estimated $\\mu(t)$ with MSE: {time_mse}\")\n",
    "plt.scatter(eval_time_points, [-config_postmean.mean_rev]*num_time_points, color=\"orange\", label=\"True $\\mu(t)$\")\n",
    "plt.title(\"IID Path Estimation of $\\mu(t)$\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylabel(\"Time-component of Drift\", fontsize=15)\n",
    "plt.xlabel(\"Real Time\", fontsize=15)\n",
    "ax.tick_params(labelsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathid in range(0, opt_drift_est.shape[0], 100):\n",
    "    path = postMean_prevPaths[pathid, :]\n",
    "    td = true_drift[pathid, :]\n",
    "    sb_drift_est = opt_drift_est[pathid, :]\n",
    "    drift_estimator = drift_hats[1:]*path[1:]\n",
    "    drift_estimator_mse = round(np.mean(np.power(td[1:]-drift_estimator,2)), 3)\n",
    "    sb_d_mse = round(np.mean(np.power(td-sb_drift_est,2)), 3)\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    plt.scatter(path, td, color=\"orange\", label=\"True Drift\")\n",
    "    plt.scatter(path[:-1], drift_estimator, color=\"green\", label=f\"Drift Kernel Estimate: MSE {round(drift_estimator_mse, 3)}\")\n",
    "    plt.scatter(path, sb_drift_est, color=\"blue\", label=f\"Score-Based Drift Estimator: MSE {round(sb_d_mse, 3)}\")\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(\"$X_{t}$\", fontsize=15)\n",
    "    plt.ylabel(\"Drift Estimate\", fontsize=15)\n",
    "    plt.title(\"Comparison of Drift Estimators\", fontsize=15)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "from sde_learn.methods.kde import ProbaDensityEstimator\n",
    "from sde_learn.methods.fp_estimator import FPEstimator\n",
    "from sde_learn.utils.data import (\n",
    "    euler_maruyama,\n",
    "    ornstein_uhlenbeck,\n",
    "    ornstein_uhlenbeck_pdf,\n",
    "    plot_map_1d,\n",
    "    plot_paths_1d,\n",
    "    line_plot,\n",
    ")\n",
    "\n",
    "\n",
    "# Simulation parameters.\n",
    "n = 1  # Dimension of the state variable\n",
    "mu = np.array([0.] * n)  # Mean\n",
    "theta = 0.8  # Rate of mean reversion\n",
    "sigma = 1#0.5 * theta ** (1 / 2)  # Volatility\n",
    "mean_0, std_0 = np.array([0.] * n), sigma / (2 * theta) ** (1 / 2)  # Mean and std at t=0\n",
    "T = 1  # Ending time\n",
    "n_steps = 1024  # Number of time steps\n",
    "dt = T / n_steps  # Time step\n",
    "T_tr = (np.arange(0, n_steps) * dt).reshape(-1, 1)  # Temporal discretization\n",
    "n_paths = 10  # Number of paths to draw\n",
    "\n",
    "# Get drift and diffusion coefficients for the Ornstein–Uhlenbeck process.\n",
    "b, sigma_func = ornstein_uhlenbeck(mu=mu, theta=theta, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a training data set of sample paths from the SDE associated to the provided coefficients.\n",
    "X_tr, drift_paths, diff_paths = euler_maruyama(b, sigma_func, n_steps, n_paths, T, n, mean_0, std_0, coef_save=True, time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data set.\n",
    "n_plot = 10\n",
    "save_path = \"...\"\n",
    "plot_paths_1d(T_tr, X_tr[:n_plot], save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probability density estimator.\n",
    "kde = ProbaDensityEstimator()\n",
    "\n",
    "# Choose the hyperparameters for the probability density estimator.\n",
    "gamma_t = 1.0\n",
    "L_t = 0.00017782794100389227\n",
    "mu_x = 10.0\n",
    "c_kernel = 1e-05\n",
    "kde.gamma_t = gamma_t\n",
    "kde.mu_x = mu_x\n",
    "kde.L_t = L_t\n",
    "kde.c_kernel = c_kernel\n",
    "\n",
    "# Fit the probability density estimator to the sample paths.\n",
    "kde.fit(T_tr=T_tr, X_tr=X_tr)\n",
    "\n",
    "# Generate uniform grid of test points (t,x) in [0,T] x [-beta/2, beta/2]^n for beta > 0.\n",
    "n_t_te = T_tr.shape[0]#256\n",
    "n_x_te = np.prod(X_tr.shape)#256\n",
    "dt_te = T / n_t_te\n",
    "t_interpolation = 0.#dt_te / 2  # Time offset between test and train times\n",
    "T_te = T_tr#np.array([i * dt_te + t_interpolation for i in range(n_t_te)]).reshape(-1, 1)\n",
    "x_min, x_max = X_tr[:, :, 0].min() - 1, X_tr[:, :, 0].max() + 1\n",
    "X_te = np.sort(X_tr.flatten()).reshape(-1, 1)#np.linspace(x_min, x_max, n_x_te).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape, X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Fokker-Planck matching estimator.\n",
    "estimator = FPEstimator()\n",
    "\n",
    "# Choose the hyperparameters for the Fokker-Planck matching estimator.\n",
    "gamma_z = 0.1\n",
    "c_kernel_z = 1e-05\n",
    "la = 1e-05\n",
    "estimator.gamma_z = gamma_z\n",
    "estimator.la = la\n",
    "estimator.be = x_max - x_min\n",
    "estimator.T = T\n",
    "estimator.c_kernel = c_kernel\n",
    "\n",
    "# Generate training points (t,x) uniformly in [0,T] x [-beta/2, beta/2]^n.\n",
    "#n_t_fp = 50\n",
    "#n_fp = 50\n",
    "#T_fp = np.random.uniform(0, T, size=(n_t_fp, 1))\n",
    "#X_fp = np.random.uniform(x_min, x_max, size=(n_fp, 1))\n",
    "\n",
    "# Fit the  Fokker-Planck matching estimator with the training samples.\n",
    "def p(Ts, X, partial):\n",
    "    return kde.predict(Ts, X, partial=partial)\n",
    "\n",
    "\n",
    "estimator.fit(T_tr=T_te, X_tr=X_te, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability density on the test set.\n",
    "p_pred = kde.predict(T_te=T_te, X_te=X_te)\n",
    "p_true = ornstein_uhlenbeck_pdf(\n",
    "    T_te, X_te.reshape((-1, 1, 1)), mu, theta, sigma, mean_0, std_0\n",
    ")\n",
    "\n",
    "# Plot the true probability density values.\n",
    "xlabel = \"$x$\"\n",
    "ylabel = \"\"\n",
    "save_path = os.curdir\n",
    "save_name = \"p_true\"\n",
    "title = r\"True density - $p$\"\n",
    "alt_label = r\"$t$\"\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=p_true,\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "# Plot the estimated probability density values.\n",
    "xlabel = \"$x$\"\n",
    "ylabel = \"\"\n",
    "save_name = f\"p_pred\"\n",
    "title = r\"Estimated density - $\\hat{p}$\"\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=p_pred,\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "# Plot both.\n",
    "xlabel = \"$x$\"\n",
    "ylabel = \"\"\n",
    "save_name = f\"p_pred_p_true\"\n",
    "title = r\"True and estimated densities - $p$ and $\\hat{p}$\"\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=r\"Time  $t$\",\n",
    "    map1=p_true,\n",
    "    map2=p_pred,\n",
    "    save_path=save_path,\n",
    "    legend1=\"p(t,x)\",\n",
    "    legend2=r\"$\\hat{p}(t,x)$\",\n",
    ")\n",
    "\n",
    "# Plot both (with varying t on the x-axis, for several fixed x).\n",
    "xlabel = \"$t$\"\n",
    "ylabel = \"\"\n",
    "save_name = f\"p_true_fixed_x\"\n",
    "title = r\"True density - $p$\"\n",
    "alt_label = r\"$x$\"\n",
    "plot_map_1d(\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    T_te,\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=p_true.T,\n",
    "    save_path=save_path,\n",
    ")\n",
    "save_name = f\"p_pred_fixed_x\"\n",
    "title = r\"Estimated density -$\\hat p$\"\n",
    "plot_map_1d(\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    T_te,\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=p_pred.T,\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "# Plot both.\n",
    "save_name = f\"p_both_fixed_x\"\n",
    "title = r\"True and estimated densities\"\n",
    "plot_map_1d(\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    T_te,\n",
    "    save_name,\n",
    "    title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=p_true.T,\n",
    "    map2=p_pred.T,\n",
    "    save_path=save_path,\n",
    "    legend1=\"p(t,x)\",\n",
    "    legend2=r\"$\\hat{p}(t,x)$\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values of the SDE's coefficients on the test set.\n",
    "B_pred_pos, S_pred_pos, B_pred, S_pred = estimator.predict(T_te=T_te, X_te=X_te)\n",
    "S_pred = S_pred.reshape((n_t_te, n_x_te))\n",
    "B_pred = B_pred.reshape((n_t_te, n_x_te))\n",
    "S_pred_pos = S_pred_pos.reshape((n_t_te, n_x_te))\n",
    "B_pred_pos = B_pred_pos.reshape((n_t_te, n_x_te))\n",
    "\n",
    "# Plot these values.\n",
    "xlabel = \"$x$\"\n",
    "ylabel = \"\"\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"sigma_both\",\n",
    "    r\"$\\sigma^2$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=S_pred,\n",
    "    map2=S_pred_pos,\n",
    "    save_path=save_path,\n",
    "    legend1=\"No cons.\",\n",
    "    legend2=r\"Pos. cons.\",\n",
    ")\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"b_both\",\n",
    "    r\"$b$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=B_pred,\n",
    "    map2=B_pred_pos,\n",
    "    save_path=save_path,\n",
    "    legend1=\"No cons.\",\n",
    "    legend2=r\"Pos. cons.\",\n",
    ")\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"sigma_pos\",\n",
    "    r\"$\\sigma^2$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=r\"$t$\",\n",
    "    map1=S_pred_pos,\n",
    "    save_path=save_path,\n",
    ")\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"b_pos\",\n",
    "    r\"$b$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=B_pred_pos,\n",
    "    save_path=save_path,\n",
    ")\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"sigma\",\n",
    "    r\"$\\sigma^2$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=S_pred,\n",
    "    save_path=save_path,\n",
    ")\n",
    "plot_map_1d(\n",
    "    T_te,\n",
    "    X_te.reshape((-1, 1, 1)),\n",
    "    f\"b\",\n",
    "    r\"$b$\",\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    alt_label=alt_label,\n",
    "    map1=B_pred,\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "# Generate a set of sample paths from the SDE associated to the estimated coefficients (b, sigma).\n",
    "print(\"Start sampling\")\n",
    "n_paths = 100\n",
    "n_steps = 100\n",
    "\n",
    "\n",
    "def b(t, x):\n",
    "    b_pred = estimator.predict(\n",
    "        T_te=np.array(t).reshape(1, 1), X_te=np.array(x).reshape(1, 1)\n",
    "    )[0]\n",
    "    return b_pred\n",
    "\n",
    "\n",
    "def sigma_func(t, x):\n",
    "    s_pred = estimator.predict(\n",
    "        T_te=np.array(t).reshape(1, 1),\n",
    "        X_te=np.array(x).reshape(1, 1),\n",
    "        thresholding=True,\n",
    "    )[1]\n",
    "    return s_pred\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "paths_pos = euler_maruyama(\n",
    "    b, sigma_func, n_steps, n_paths, T, n, mean_0, std_0, time=True\n",
    ")\n",
    "print(f\"End sampling\")\n",
    "print(f\"Sampling computation time: {time.time() - t0}\")\n",
    "\n",
    "# Plot the set.\n",
    "dt = T / n_steps\n",
    "T_samp = (np.arange(0, n_steps) * dt).reshape(-1, 1)\n",
    "save_path = os.curdir\n",
    "plot_paths_1d(T_samp, paths_pos, save_path=save_path)\n",
    "\n",
    "# Compute and plot mean and variance of estimated SDE w.r.t. time.\n",
    "mean_pos = np.mean(paths_pos, axis=0)\n",
    "std_pos = np.std(paths_pos, axis=0)\n",
    "save_path = os.curdir\n",
    "line_plot(T_samp, mean_pos, save_path, title=r\"$\\hat \\mu(t)$\")\n",
    "save_path = os.curdir\n",
    "line_plot(T_samp, std_pos, save_path, title=r\"$\\hat \\sigma(t)$\")\n",
    "\n",
    "# Compute and plot mean and variance of true SDE w.r.t. time.\n",
    "mean_true = np.exp(-theta * T_samp) * (mean_0 - mu) + mu\n",
    "var_true = (sigma**2 / (2 * theta)) * (1 - np.exp(-2 * theta * T_samp)) + np.exp(\n",
    "    -2 * theta * T_samp\n",
    ") * std_0**2\n",
    "std_true = var_true ** (1 / 2)\n",
    "save_path = os.curdir\n",
    "title = r\"$\\mu(t)$\"\n",
    "line_plot(T_samp, mean_true, save_path, title)\n",
    "save_path = os.curdir\n",
    "title = r\"$\\sigma(t)$\"\n",
    "line_plot(T_samp, std_true, save_path, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
