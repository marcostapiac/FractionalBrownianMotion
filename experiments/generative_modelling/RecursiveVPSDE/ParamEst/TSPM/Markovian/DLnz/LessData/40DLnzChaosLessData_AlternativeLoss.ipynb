{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/marcos/GitHubRepos/FractionalBrownianMotion/venv/lib/python3.10/site-packages/namerepo_root.pth\n"
     ]
    }
   ],
   "source": [
    "import site, pathlib, subprocess, sys\n",
    "sys.path.insert(0,\"/home/mt622/GitHubRepos/FractionalBrownianMotion\")\n",
    "repo_root = subprocess.check_output(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"], text=True\n",
    ").strip()\n",
    "pth_dir = pathlib.Path(site.getsitepackages()[0])\n",
    "(pth_dir / \"namerepo_root.pth\").write_text(repo_root + \"\\n\")\n",
    "print(\"Wrote\", pth_dir / \"namerepo_root.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import shared_memory\n",
    "from src.classes.ClassFractionalLorenz96 import FractionalLorenz96\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.generative_modelling.models.ClassVPSDEDiffusion import VPSDEDiffusion\n",
    "from src.generative_modelling.models.TimeDependentScoreNetworks.ClassConditionalMarkovianTSPostMeanScoreMatching import \\\n",
    "    ConditionalMarkovianTSPostMeanScoreMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "from configs.RecursiveVPSDE.Markovian_40DLorenz.recursive_Markovian_PostMeanScore_40DLorenz_Chaos_T256_H05_tl_110data_StbleTgt import get_config\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcos/GitHubRepos/FractionalBrownianMotion/src/generative_modelling/trained_models/trained_rec_PM_ST_0100FTh_MLP_2LFac_NSTgtNFMReg_40DLnz_125e+00FConst_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_MLP_H4_CUp20_tl110 40000\n"
     ]
    }
   ],
   "source": [
    "assert (config.hurst == 0.5)\n",
    "assert (config.early_stop_idx == 0)\n",
    "assert (config.tdata_mult == 110)\n",
    "#assert (config.sin_space_scale == 25.)\n",
    "assert (config.feat_thresh != 1./1.)\n",
    "print(config.scoreNet_trained_path, config.dataSize)\n",
    "rng = np.random.default_rng()\n",
    "diffusion = VPSDEDiffusion(beta_max=config.beta_max, beta_min=config.beta_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_rec_PM_ST_0100FTh_MLP_2LFac_NSTgtNFMReg_40DLnz_125e+00FConst_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_MLP_H4_CUp20_tl110_NEp3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/\".join(config.scoreNet_trained_path.split(\"/\")[:-1]) + \"/\"\n",
    "entered = False\n",
    "for file in os.listdir(model_dir):\n",
    "    if config.scoreNet_trained_path in os.path.join(model_dir, file) and (\"Trk\" not in file):\n",
    "        good = ConditionalMarkovianTSPostMeanScoreMatching(\n",
    "    *config.model_parameters)\n",
    "        print(file)\n",
    "        entered = True\n",
    "        good.load_state_dict(torch.load(os.path.join(model_dir, file)))\n",
    "        try:\n",
    "            best_epoch = int(file.split(\"Trk\")[-1][3:])\n",
    "        except ValueError as e:\n",
    "            best_epoch = int(file.split(\"NEp\")[-1])\n",
    "assert entered\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def true_drift(prev, num_paths, config):\n",
    "    assert (prev.shape == (num_paths, config.ndims))\n",
    "    drift_X = np.zeros((num_paths, config.ndims))\n",
    "    for i in range(config.ndims):\n",
    "        drift_X[:, i] = (prev[:, (i + 1) % config.ndims] - prev[:, i - 2]) * prev[:, i - 1] - prev[:,\n",
    "                                                                                              i] *config.forcing_const\n",
    "    return drift_X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def experiment_MLP_1D_drifts(config, es,Xs, good, onlyGauss=False):\n",
    "    if config.has_cuda:\n",
    "        device = 0#int(os.environ[\"LOCAL_RANK\"])\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    Xs = torch.Tensor(Xs).to(device)\n",
    "    good = good.to(device)\n",
    "    diffusion = VPSDEDiffusion(beta_max=config.beta_max, beta_min=config.beta_min)\n",
    "    ts_step = config.deltaT\n",
    "    Xshape = Xs.shape[0]\n",
    "    num_taus = 10\n",
    "\n",
    "    num_diff_times = config.max_diff_steps\n",
    "    Ndiff_discretisation = config.max_diff_steps\n",
    "    diffusion_times = torch.linspace(start=config.sample_eps, end=config.end_diff_time,\n",
    "                                     steps=Ndiff_discretisation).to(device)\n",
    "\n",
    "    features_tensor = torch.stack([Xs for _ in range(1)], dim=0).reshape(Xshape * 1, 1, -1).to(device)\n",
    "    vec_Z_taus = diffusion.prior_sampling(shape=(Xshape * num_taus, 1, config.ts_dims)).to(device)\n",
    "\n",
    "    # ts = []\n",
    "    es = num_diff_times - es\n",
    "    final_vec_mu_hats = np.zeros(\n",
    "        (Xshape, es, num_taus, config.ts_dims))  # Xvalues, DiffTimes, Ztaus, Ts_Dims\n",
    "\n",
    "    ts = []\n",
    "    # mu_hats_mean = np.zeros((tot_num_feats, num_taus))\n",
    "    # mu_hats_std = np.zeros((tot_num_feats, num_taus))\n",
    "    good.eval()\n",
    "    insert_idx=-1\n",
    "    for difftime_idx in (np.arange(num_diff_times - 1, num_diff_times - es - 1, -1)): #difftime_idx >= num_diff_times - es:\n",
    "        d = diffusion_times[Ndiff_discretisation - (num_diff_times - 1 - difftime_idx) - 1].to(device)\n",
    "        diff_times = torch.stack([d for _ in range(Xshape)]).reshape(Xshape * 1).to(device)\n",
    "        eff_times = diffusion.get_eff_times(diff_times=diff_times).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "        vec_diff_times = torch.stack([diff_times for _ in range(num_taus)], dim=0).reshape(num_taus * Xshape)\n",
    "        vec_eff_times = torch.stack([eff_times for _ in range(num_taus)], dim=0).reshape(num_taus * Xshape, 1, 1)\n",
    "        vec_conditioner = torch.stack([features_tensor for _ in range(num_taus)], dim=0).reshape(\n",
    "            num_taus * Xshape,\n",
    "            1, -1)\n",
    "        with torch.no_grad():\n",
    "            if onlyGauss:\n",
    "                scoreEval_vec_Z_taus = torch.randn_like(vec_Z_taus).to(device)\n",
    "            else:\n",
    "                scoreEval_vec_Z_taus = vec_Z_taus\n",
    "            vec_predicted_score = good.forward(inputs=scoreEval_vec_Z_taus, times=vec_diff_times, conditioner=vec_conditioner,\n",
    "                                             eff_times=vec_eff_times)\n",
    "\n",
    "        beta_taus = torch.exp(-0.5 * eff_times[0, 0, 0]).to(device)\n",
    "        sigma_taus = torch.pow(1. - torch.pow(beta_taus, 2), 0.5).to(device)\n",
    "        sigma2_taus=torch.pow(1. - torch.pow(beta_taus, 2), 1.).to(device)\n",
    "        predicted_score = -scoreEval_vec_Z_taus/sigma2_taus + (beta_taus/sigma2_taus)*vec_predicted_score\n",
    "        vec_scores, vec_drift, vec_diffParam = diffusion.get_conditional_reverse_diffusion(x=vec_Z_taus,\n",
    "                                                                                           predicted_score=predicted_score,\n",
    "                                                                                           diff_index=torch.Tensor(\n",
    "                                                                                               [int((\n",
    "                                                                                                       num_diff_times - 1 - difftime_idx))]).to(\n",
    "                                                                                               device),\n",
    "                                                                                           max_diff_steps=Ndiff_discretisation)\n",
    "\n",
    "        if \"PM\" in config.scoreNet_trained_path:\n",
    "            final_mu_hats = (beta_taus*scoreEval_vec_Z_taus / (sigma2_taus)) + ((\n",
    "                                                                            (torch.pow(sigma_taus, 2) + (\n",
    "                                                                                    torch.pow(beta_taus * config.diffusion,\n",
    "                                                                                            2) * ts_step)) / (\n",
    "                                                                                    ts_step * sigma2_taus)) * vec_predicted_score)\n",
    "        else:\n",
    "            final_mu_hats = (scoreEval_vec_Z_taus / (ts_step * beta_taus)) + ((\n",
    "                                                                            (sigma2_taus + (\n",
    "                                                                                    torch.pow(beta_taus * config.diffusion,\n",
    "                                                                                            2) * ts_step)) / (\n",
    "                                                                                    ts_step * beta_taus)) * vec_scores)\n",
    "\n",
    "        assert (final_mu_hats.shape == (num_taus * Xshape, 1, config.ts_dims))\n",
    "        means = final_mu_hats.reshape((num_taus, Xshape, config.ts_dims))\n",
    "\n",
    "        # print(vec_Z_taus.shape, vec_scores.shape)\n",
    "        final_vec_mu_hats[:, insert_idx,:, :] = means.permute((1, 0, 2)).cpu().numpy()\n",
    "        vec_z = torch.randn_like(vec_drift).to(device)\n",
    "        vec_Z_taus = vec_drift + vec_diffParam * vec_z\n",
    "        insert_idx -=1\n",
    "    assert (final_vec_mu_hats.shape == (Xshape, es, num_taus, config.ts_dims))\n",
    "    return final_vec_mu_hats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def _get_device(device_str: str | None = None):\n",
    "    if device_str is not None:\n",
    "        return torch.device(device_str)\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def true_drift_gpu(prev: torch.Tensor, num_paths: int, config) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    prev: (num_paths, d) float32 CUDA tensor\n",
    "    returns: (num_paths, 1, d) float32 CUDA tensor\n",
    "    \"\"\"\n",
    "    # Ensure shape\n",
    "    assert prev.ndim == 2 and prev.shape[0] == num_paths and prev.shape[1] == config.ndims\n",
    "    x = prev\n",
    "    # Vectorized Lorenz96 drift:\n",
    "    x_ip1 = torch.roll(x, shifts=-1, dims=1)  # x_{i+1}\n",
    "    x_im1 = torch.roll(x, shifts=1,  dims=1)  # x_{i-1}\n",
    "    x_im2 = torch.roll(x, shifts=2,  dims=1)  # x_{i-2}\n",
    "    drift = (x_ip1 - x_im2) * x_im1 - x*float(config.forcing_const)\n",
    "    return drift[:, None, :]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def IID_NW_multivar_estimator_gpu(\n",
    "    prevPath_observations: torch.Tensor,  # (N,n,d) float32 CUDA\n",
    "    path_incs: torch.Tensor,              # (N,n,d) float32 CUDA\n",
    "    inv_H: torch.Tensor,                  # (d,) (diag) or (d,d) float32 CUDA\n",
    "    norm_const: float,                    # same meaning as your CPU code\n",
    "    x: torch.Tensor,                      # (M,d) float32 CUDA\n",
    "    t1: float,\n",
    "    t0: float,\n",
    "    truncate: bool = True,\n",
    "    M_tile: int = 32,                     # micro-batch states\n",
    "    Nn_tile: int | None = 512_000,        # micro-batch samples (None => full)\n",
    "    stable: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns: (M,d) float32 CUDA tensor (keeps all heavy ops on LongerTimes_GPU).\n",
    "    Matches your scaling:\n",
    "      denom = sum(w)/(N*n)\n",
    "      numer = (sum(w * incs)/N) * (t1 - t0)\n",
    "    \"\"\"\n",
    "    #assert prevPath_observations.is_cuda and path_incs.is_cuda and x.is_cuda\n",
    "    assert prevPath_observations.dtype == torch.float32\n",
    "    assert path_incs.dtype == torch.float32\n",
    "    assert x.dtype == torch.float32\n",
    "\n",
    "    N, n, d = prevPath_observations.shape\n",
    "    Nn = N * n\n",
    "    if Nn_tile is None or Nn_tile > Nn:\n",
    "        Nn_tile = Nn\n",
    "\n",
    "    # Flatten once\n",
    "    mu = prevPath_observations.reshape(Nn, d).contiguous()  # (Nn,d)\n",
    "    dX = path_incs.reshape(Nn, d).contiguous()              # (Nn,d)\n",
    "\n",
    "    # Diagonal vs full inv_H\n",
    "    diag = (inv_H.ndim == 1)\n",
    "    if diag:\n",
    "        A = inv_H                                           # (d,)\n",
    "        muAh = mu * A                                       # (Nn,d)\n",
    "        mu_quad = (mu * muAh).sum(-1)                       # (Nn,)\n",
    "        def xAh(X): return X * A\n",
    "    else:\n",
    "        A = inv_H                                           # (d,d)\n",
    "        muAh = mu @ A                                       # (Nn,d)\n",
    "        mu_quad = (mu * muAh).sum(-1)                       # (Nn,)\n",
    "        # Sanity: PD\n",
    "        sign, _ = torch.linalg.slogdet(A)\n",
    "        if sign.item() <= 0:\n",
    "            raise ValueError(\"inv_H must be positive definite.\")\n",
    "\n",
    "    # Use log(norm_const) directly to match your CPU estimator\n",
    "    log_norm_const = float(np.log(norm_const))\n",
    "\n",
    "    M = x.size(0)\n",
    "    # Accumulate in float64 for stability\n",
    "    denom = torch.zeros(M,     dtype=torch.float64, device=x.device)\n",
    "    numer = torch.zeros(M, d,  dtype=torch.float64, device=x.device)\n",
    "\n",
    "    for m0 in range(0, M, M_tile):\n",
    "        X = x[m0:m0 + M_tile]                     # (mb,d)\n",
    "        XAh = xAh(X)                               # (mb,d)\n",
    "        X_quad = (X * XAh).sum(-1)                 # (mb,)\n",
    "\n",
    "        denom_tile = torch.zeros(X.size(0),    dtype=torch.float64, device=x.device)\n",
    "        numer_tile = torch.zeros(X.size(0), d, dtype=torch.float64, device=x.device)\n",
    "\n",
    "        if stable:\n",
    "            lse_max = torch.full((X.size(0),), -torch.inf, dtype=torch.float32, device=x.device)\n",
    "            # First pass: find max exponent per state (over all Nn tiles)\n",
    "            for i0 in range(0, Nn, Nn_tile):\n",
    "                muq_i  = mu_quad[i0:i0 + Nn_tile]             # (bn,)\n",
    "                muAh_i = muAh[i0:i0 + Nn_tile]                # (bn,d)\n",
    "                cross  = muAh_i @ X.t()                       # (bn,mb)\n",
    "                expo   = log_norm_const - 0.5 * (muq_i[:, None] + X_quad[None, :] - 2.0 * cross)\n",
    "                lse_max = torch.maximum(lse_max, expo.max(dim=0).values)\n",
    "\n",
    "        # Second pass: accumulate with stabilization (or plain)\n",
    "        for i0 in range(0, Nn, Nn_tile):\n",
    "            muAh_i = muAh[i0:i0 + Nn_tile]                    # (bn,d)\n",
    "            muq_i  = mu_quad[i0:i0 + Nn_tile]                 # (bn,)\n",
    "            dX_i   = dX[i0:i0 + Nn_tile]                      # (bn,d)\n",
    "\n",
    "            cross = muAh_i @ X.t()                            # (bn,mb)\n",
    "            expo  = log_norm_const - 0.5 * (muq_i[:, None] + X_quad[None, :] - 2.0 * cross)\n",
    "\n",
    "            if stable:\n",
    "                w = torch.exp(expo - lse_max[None, :])        # (bn,mb)\n",
    "            else:\n",
    "                w = torch.exp(expo)\n",
    "\n",
    "            denom_tile += w.sum(dim=0, dtype=torch.float64) / (N * n)\n",
    "            numer_tile += (w.t() @ dX_i).to(torch.float64) * ((t1 - t0) / N)\n",
    "\n",
    "        if stable:\n",
    "            scale = torch.exp(lse_max.to(torch.float64))\n",
    "            denom_tile *= scale\n",
    "            numer_tile *= scale[:, None]\n",
    "\n",
    "        denom[m0:m0 + X.size(0)] += denom_tile\n",
    "        numer[m0:m0 + X.size(0)] += numer_tile\n",
    "\n",
    "    est = (numer / denom[:, None]).to(torch.float32)          # (M,d)\n",
    "\n",
    "    if truncate:\n",
    "        m = denom.min()\n",
    "        est[denom <= (m / 2.0)] = 0\n",
    "\n",
    "    return est\n",
    "\n",
    "def run_Nadaraya(config, Xs):\n",
    "    num_paths = 1024 if config.feat_thresh == 1. else 1024\n",
    "    assert num_paths == 1024\n",
    "    t0 = config.t0\n",
    "    deltaT = config.deltaT\n",
    "    t1 = deltaT * config.ts_length\n",
    "    # Drift parameters\n",
    "    diff = config.diffusion\n",
    "    initial_state = np.array(config.initState)\n",
    "    rvs = None\n",
    "    H = config.hurst\n",
    "    try:\n",
    "        is_path_observations = np.load(config.data_path, allow_pickle=True)[:num_paths, :, :]\n",
    "        is_path_observations = np.concatenate(\n",
    "            [np.repeat(np.array(config.initState).reshape((1, 1, config.ndims)), is_path_observations.shape[0], axis=0),\n",
    "             is_path_observations], axis=1)\n",
    "        assert is_path_observations.shape == (num_paths, config.ts_length + 1, config.ndims)\n",
    "    except (FileNotFoundError, AssertionError) as e:\n",
    "        print(e)\n",
    "        fLnz = FractionalLorenz96(X0=config.initState, diff=config.diffusion, num_dims=config.ndims,\n",
    "                                  forcing_const=config.forcing_const)\n",
    "        is_path_observations = np.array(\n",
    "            [fLnz.euler_simulation(H=H, N=config.ts_length, deltaT=deltaT, X0=initial_state, Ms=None, gaussRvs=rvs,\n",
    "                                   t0=t0, t1=t1) for _ in (range(num_paths))]).reshape(\n",
    "            (num_paths, config.ts_length + 1, config.ndims))\n",
    "        np.save(config.data_path, is_path_observations[:, 1:, :])\n",
    "        assert is_path_observations.shape == (num_paths, config.ts_length + 1, config.ndims)\n",
    "\n",
    "    is_idxs = np.arange(is_path_observations.shape[0])\n",
    "    path_observations = is_path_observations[np.random.choice(is_idxs, size=num_paths, replace=False), :]\n",
    "    # We note that we DO NOT evaluate the drift at time t_{0}=0\n",
    "    # We therefore remove the first element of path_observations since it includes X_{t_{0}} = X_{0}\n",
    "    # We also remove the last element since we never evaluate the drift at that point\n",
    "    t0 = deltaT\n",
    "    prevPath_observations = path_observations[:, 1:-1, :]\n",
    "    # We compute the path incs with respect to the prevPath_observations (since X_{t_{0}} != X_{0})\n",
    "    path_incs = np.diff(path_observations, axis=1)[:, 1:, :]\n",
    "    assert (prevPath_observations.shape == path_incs.shape)\n",
    "    assert (path_incs.shape[1] == config.ts_length - 1)\n",
    "    assert (path_observations.shape[1] == prevPath_observations.shape[1] + 2)\n",
    "    assert (prevPath_observations.shape[1] * deltaT == (t1 - t0))\n",
    "    grid_1d = np.logspace(-3.55, -0.05, 30)\n",
    "    xadd = np.logspace(-0.05, 1.0, 11)[1:]  # 10 values > -0.05\n",
    "    grid_1d = np.concatenate([grid_1d, xadd])\n",
    "    bws = np.stack([grid_1d for m in range(config.ndims)], axis=-1)\n",
    "    assert (bws.shape == (40, config.ndims))\n",
    "\n",
    "    prevPath_shm = shared_memory.SharedMemory(create=True, size=prevPath_observations.nbytes)\n",
    "    path_incs_shm = shared_memory.SharedMemory(create=True, size=path_incs.nbytes)\n",
    "\n",
    "    # Create numpy arrays from the shared memory buffers\n",
    "    prevPath_shm_array = np.ndarray(prevPath_observations.shape, dtype=np.float64, buffer=prevPath_shm.buf)\n",
    "    path_incs_shm_array = np.ndarray(path_incs.shape, dtype=np.float64, buffer=path_incs_shm.buf)\n",
    "\n",
    "    # Copy the data into the shared memory arrays\n",
    "    np.copyto(prevPath_shm_array, prevPath_observations)\n",
    "    np.copyto(path_incs_shm_array, path_incs)\n",
    "\n",
    "    # Ensure randomness across starmap calls\n",
    "    master_seed = 42\n",
    "    seed_seq = np.random.SeedSequence(master_seed)\n",
    "\n",
    "    # Euler-Maruyama Scheme for Tracking Errors\n",
    "    shape = prevPath_observations.shape\n",
    "    M_tile = 32\n",
    "    Nn_tile = 512000\n",
    "    stable = True\n",
    "    bw_idx = 30\n",
    "    bw = bws[bw_idx, :]\n",
    "    inv_H = np.diag(np.power(bw, -2))\n",
    "    norm_const = 1 / np.sqrt((2. * np.pi) ** config.ndims * (1. / np.linalg.det(inv_H)))\n",
    "    num_dhats = 10\n",
    "    device = _get_device(\"cpu\")\n",
    "\n",
    "    # Upload once (float32)\n",
    "    unif_is_drift_hats = np.zeros((Xshape, num_dhats, config.ts_dims))\n",
    "    Xs = torch.as_tensor(Xs, dtype=torch.float32, device=device).contiguous()\n",
    "    for k in (range(num_dhats)):\n",
    "        is_ss_path_observations = is_path_observations[np.random.choice(is_idxs, size=num_paths, replace=False), :]\n",
    "        is_prevPath_observations = is_ss_path_observations[:, 1:-1]\n",
    "        is_path_incs = np.diff(is_ss_path_observations, axis=1)[:, 1:]\n",
    "        is_prevPath_observations = torch.as_tensor(is_prevPath_observations, dtype=torch.float32, device=device).contiguous()\n",
    "        is_path_incs = torch.as_tensor(is_path_incs, dtype=torch.float32, device=device).contiguous()\n",
    "        # inv_H: prefer diagonal vector if possible\n",
    "        inv_H_np = np.asarray(inv_H)\n",
    "        if inv_H_np.ndim == 2 and np.allclose(inv_H_np, np.diag(np.diag(inv_H_np))):\n",
    "            inv_H_vec = np.diag(inv_H_np).astype(np.float32)\n",
    "            inv_H = torch.as_tensor(inv_H_vec, device=device)\n",
    "        else:\n",
    "            inv_H = torch.as_tensor(inv_H_np.astype(np.float32), device=device)\n",
    "\n",
    "        unif_is_drift_hats[:, k, :]  =  IID_NW_multivar_estimator_gpu(\n",
    "            is_prevPath_observations, is_path_incs, inv_H, float(norm_const),\n",
    "            Xs, float(config.t1), float(config.t0),\n",
    "            truncate=True, M_tile=M_tile, Nn_tile=Nn_tile, stable=stable\n",
    "            ).numpy()\n",
    "    return unif_is_drift_hats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_file_path = \"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/\" #project_config.ROOT_DIR+\"data/\"\n",
    "remote_file_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_drift_files(config, root_dir, best_epoch):\n",
    "    ts_type = f\"{config.ndims}DLnz_\".replace(\".\", \"\")\n",
    "    include =  (f\"_{1}NDT_{config.loss_factor}LFac_BetaMax{config.beta_max:.1e}_{round(config.forcing_const,3)}FConst\").replace(\n",
    "            \".\", \"\")\n",
    "    root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/40DLnzChaosLessData/\"\n",
    "    driftoostrack_true_files = []\n",
    "    driftoostrack_local_files = []\n",
    "    for file in os.listdir(root_score_dir):\n",
    "        if \"MLP\" in file and ts_type in file and include in file and (\"_\"+str(best_epoch)+\"Nep\") in file:\n",
    "            if \"DriftTrack\" in file and \"true\" in file:\n",
    "                driftoostrack_true_files.append(root_score_dir+file)\n",
    "            elif \"DriftTrack\" in file and \"global\" in file:\n",
    "                driftoostrack_local_files.append(root_score_dir+file)\n",
    "    assert len(driftoostrack_true_files)>0, \"No oos drift track files found\"\n",
    "    assert len(driftoostrack_local_files)>0, \"No oos drift track files found\"\n",
    "    assert(len(driftoostrack_true_files) == len(driftoostrack_true_files))\n",
    "    def extract_bw_drift_track_number(s):\n",
    "        match = s.split(\"Nep_\")[0].split(\"_\")[-1]\n",
    "        return int(match)\n",
    "    driftoostrack_true_files = sorted(driftoostrack_true_files, key=extract_bw_drift_track_number)\n",
    "    driftoostrack_local_files = sorted(driftoostrack_local_files, key=extract_bw_drift_track_number)\n",
    "    Nepochs_track = [extract_bw_drift_track_number(f) for f in driftoostrack_true_files]\n",
    "    return np.load(driftoostrack_true_files[0]), driftoostrack_local_files, Nepochs_track"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "true, _, _ = get_drift_files(config=config, root_dir=remote_file_path, best_epoch=best_epoch)\n",
    "true = true.reshape(-1, config.ts_dims)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:32<00:00,  9.27s/it]\n"
     ]
    }
   ],
   "source": [
    "totN = 0\n",
    "score_sse = 0\n",
    "nad_sse = 0\n",
    "for nsamp in tqdm(range(10)):\n",
    "    true_nsamp = true[np.random.choice(np.arange(0,true.shape[0]),100),:]\n",
    "    Xshape = true_nsamp.shape[0]\n",
    "    Xs = true_nsamp\n",
    "    true_drifts = true_drift(prev=Xs, num_paths=Xs.shape[0], config=config)\n",
    "    es=config.max_diff_steps - 1\n",
    "    DNonGauss = experiment_MLP_1D_drifts(good=good, es=es, Xs=Xs, config=config, onlyGauss=False)\n",
    "    unif_is_drift_hats = run_Nadaraya(config=config, Xs=Xs)\n",
    "    totN += true_nsamp.shape[0]\n",
    "    score_sse += np.sum(np.sum(np.power(DNonGauss[:, 0, :, :].mean(axis=1) - true_drifts,2), axis=-1))\n",
    "    nad_sse += np.sum(np.sum(np.power(unif_is_drift_hats.mean(axis=1) - true_drifts,2), axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447.7020503815116 150.42973417928275\n"
     ]
    }
   ],
   "source": [
    "print(score_sse/totN, nad_sse/totN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
