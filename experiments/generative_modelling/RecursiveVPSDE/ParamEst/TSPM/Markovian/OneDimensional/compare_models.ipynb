{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from configs import project_config\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from src.generative_modelling.models.ClassVPSDEDiffusion import VPSDEDiffusion\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.RecursiveVPSDE.Markovian_fQuadSinHF.recursive_Markovian_PostMeanScore_fQuadSinHF2_LowFTh_T256_H05_tl_110data_StbleTgt import get_config\n",
    "#from configs.RecursiveVPSDE.Markovian_fSinLog.recursive_Markovian_PostMeanScore_fSinLog_LowFTh_T256_H05_tl_110data_StbleTgt import get_config\n",
    "#from configs.RecursiveVPSDE.Markovian_fBiPot.recursive_Markovian_PostMeanScore_fBiPot_LowFTh_T256_H05_tl_110data_StbleTgt\n",
    "#import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_rec_ST_0002FTh_PM_MLP_2LFac_NSTgtfQuadSinHF_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_05a_004b_250c_MLP_H4_CUp20_tl110_EENEp33\n",
      "trained_rec_ST_0002FTh_PM_MLP_2LFac_NSTgtfQuadSinHF_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_05a_004b_250c_MLP_H4_CUp20_tl110_NEp35\n"
     ]
    },
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.generative_modelling.models.TimeDependentScoreNetworks.ClassConditionalMarkovianTSPostMeanScoreMatching import \\\n",
    "    ConditionalMarkovianTSPostMeanScoreMatching\n",
    "config = get_config()\n",
    "good_scoreModel = ConditionalMarkovianTSPostMeanScoreMatching(\n",
    "            *config.model_parameters)\n",
    "model_dir = \"/\".join(config.scoreNet_trained_path.split(\"/\")[:-1]) + \"/\"\n",
    "entered = False\n",
    "for file in os.listdir(model_dir):\n",
    "    if config.scoreNet_trained_path in os.path.join(model_dir, file) and \"EE\" in file:\n",
    "        print(file)\n",
    "        entered = True\n",
    "        good_scoreModel.load_state_dict(torch.load(os.path.join(model_dir, file)))\n",
    "bad_scoreModel = ConditionalMarkovianTSPostMeanScoreMatching(\n",
    "            *config.model_parameters)\n",
    "model_dir = \"/\".join(config.scoreNet_trained_path.split(\"/\")[:-1]) + \"/\"\n",
    "entered = False\n",
    "for file in os.listdir(model_dir):\n",
    "    if config.scoreNet_trained_path in os.path.join(model_dir, file) and (\"EE\" not in file and \"Trk\" not in file):\n",
    "        print(file)\n",
    "        entered = True\n",
    "        bad_scoreModel.load_state_dict(torch.load(os.path.join(model_dir, file)))\n",
    "good_scoreModel.eval()\n",
    "bad_scoreModel.eval()\n",
    "\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0]\n",
      " [   1]\n",
      " [   2]\n",
      " ...\n",
      " [9997]\n",
      " [9998]\n",
      " [9999]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load(config.data_path, allow_pickle=True)\n",
    "size = 10000\n",
    "pathidx=np.random.randint(low=0, high=data.shape[0], size=size)\n",
    "timeidx=np.random.randint(low=0, high=data.shape[1], size=size)\n",
    "inc = data[pathidx, timeidx] - data[pathidx, timeidx - 1]\n",
    "data = data[pathidx, timeidx-1]\n",
    "idxs = np.argwhere(np.abs(data)<1.)\n",
    "times = torch.Tensor([1]*data.shape[0])\n",
    "diffusion = VPSDEDiffusion(beta_max=config.beta_max, beta_min=config.beta_min)\n",
    "eff_times = diffusion.get_eff_times(times)\n",
    "sample = {\n",
    "     \"inputs\":torch.Tensor([inc]).reshape(size, 1, 1),\n",
    "     \"times\":      times,\n",
    "     \"conditioner\": torch.Tensor([data]).reshape(size,1,1),\n",
    "     \"eff_times\":   torch.Tensor(eff_times).reshape(size, 1, 1),\n",
    " }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([  12.,   82.,  439., 1541., 3391., 2908., 1241.,  318.,   64.,\n           4.]),\n array([-2.25355983, -1.79183066, -1.33010149, -0.86837238, -0.40664324,\n         0.0550859 ,  0.51681507,  0.97854418,  1.44027328,  1.90200245,\n         2.36373162]),\n <BarContainer object of 10 artists>)"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sample[\"conditioner\"].flatten())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Weight delta report]\n",
      "||Δθ|| = 0.0943961\n",
      "cosine(flat_weights_pre, flat_weights_post) = 0.999996\n",
      "diffusion_embedding.projection1.weight         |Δ|=0.058873   rel=1.199e-02\n",
      "diffusion_embedding.projection2.weight         |Δ|=0.041878   rel=8.595e-03\n",
      "residual_layers.7.diffusion_projection.weight  |Δ|=0.018334   rel=1.082e-02\n",
      "residual_layers.4.diffusion_projection.weight  |Δ|=0.018076   rel=1.056e-02\n",
      "residual_layers.3.diffusion_projection.weight  |Δ|=0.017194   rel=1.012e-02\n",
      "residual_layers.8.diffusion_projection.weight  |Δ|=0.016889   rel=9.693e-03\n",
      "residual_layers.6.diffusion_projection.weight  |Δ|=0.016458   rel=9.741e-03\n",
      "residual_layers.0.diffusion_projection.weight  |Δ|=0.015804   rel=9.523e-03\n",
      "residual_layers.2.diffusion_projection.weight  |Δ|=0.015001   rel=8.796e-03\n",
      "residual_layers.9.diffusion_projection.weight  |Δ|=0.014738   rel=8.809e-03\n",
      "residual_layers.5.diffusion_projection.weight  |Δ|=0.013909   rel=8.154e-03\n",
      "residual_layers.1.diffusion_projection.weight  |Δ|=0.013719   rel=7.828e-03\n",
      "residual_layers.6.dilated_conv.weight          |Δ|=0.0090227   rel=3.873e-03\n",
      "residual_layers.5.dilated_conv.weight          |Δ|=0.0087815   rel=3.723e-03\n",
      "residual_layers.9.dilated_conv.weight          |Δ|=0.0084186   rel=3.569e-03\n",
      "residual_layers.7.dilated_conv.weight          |Δ|=0.007799   rel=3.379e-03\n",
      "residual_layers.8.dilated_conv.weight          |Δ|=0.0076819   rel=3.278e-03\n",
      "residual_layers.1.dilated_conv.weight          |Δ|=0.0073443   rel=3.277e-03\n",
      "residual_layers.0.dilated_conv.weight          |Δ|=0.0070118   rel=3.035e-03\n",
      "residual_layers.3.dilated_conv.weight          |Δ|=0.0068317   rel=2.864e-03\n",
      "residual_layers.4.dilated_conv.weight          |Δ|=0.0065709   rel=2.894e-03\n",
      "residual_layers.2.dilated_conv.weight          |Δ|=0.006374   rel=2.705e-03\n",
      "residual_layers.5.output_projection.weight     |Δ|=0.0061593   rel=1.111e-03\n",
      "residual_layers.4.output_projection.weight     |Δ|=0.0061089   rel=1.214e-03\n",
      "residual_layers.0.output_projection.weight     |Δ|=0.0058597   rel=1.099e-03\n",
      "residual_layers.1.output_projection.weight     |Δ|=0.0058578   rel=9.912e-04\n",
      "residual_layers.6.output_projection.weight     |Δ|=0.0056783   rel=9.922e-04\n",
      "residual_layers.2.output_projection.weight     |Δ|=0.005645   rel=1.055e-03\n",
      "diffusion_embedding.projection1.bias           |Δ|=0.0055167   rel=1.330e-02\n",
      "residual_layers.3.output_projection.weight     |Δ|=0.0052973   rel=9.511e-04\n",
      "skip_projection.weight                         |Δ|=0.005045   rel=1.370e-03\n",
      "residual_layers.8.output_projection.weight     |Δ|=0.0046736   rel=8.087e-04\n",
      "residual_layers.7.output_projection.weight     |Δ|=0.0046513   rel=9.484e-04\n",
      "diffusion_embedding.projection2.bias           |Δ|=0.0042796   rel=6.870e-03\n",
      "residual_layers.9.output_projection.weight     |Δ|=0.0040527   rel=7.073e-04\n",
      "cond_upsampler.linear1.weight                  |Δ|=0.0039265   rel=1.499e-03\n",
      "mlp_state_mapper.linear2.weight                |Δ|=0.0030523   rel=2.605e-03\n",
      "mlp_state_mapper.linear3.weight                |Δ|=0.0022351   rel=9.410e-04\n",
      "residual_layers.4.output_projection.bias       |Δ|=0.0021817   rel=3.352e-03\n",
      "residual_layers.0.conditioner_projection.bias  |Δ|=0.0018628   rel=1.006e-03\n",
      "residual_layers.0.dilated_conv.bias            |Δ|=0.0018628   rel=3.920e-03\n",
      "residual_layers.5.dilated_conv.bias            |Δ|=0.0018334   rel=3.484e-03\n",
      "residual_layers.5.conditioner_projection.bias  |Δ|=0.0018334   rel=1.291e-03\n",
      "residual_layers.9.conditioner_projection.bias  |Δ|=0.0017906   rel=6.845e-04\n",
      "residual_layers.9.dilated_conv.bias            |Δ|=0.0017906   rel=3.621e-03\n",
      "residual_layers.3.output_projection.bias       |Δ|=0.0017741   rel=2.840e-03\n",
      "residual_layers.6.output_projection.bias       |Δ|=0.0017419   rel=2.093e-03\n",
      "residual_layers.2.output_projection.bias       |Δ|=0.0017317   rel=1.919e-03\n",
      "residual_layers.3.dilated_conv.bias            |Δ|=0.0017087   rel=3.796e-03\n",
      "residual_layers.3.conditioner_projection.bias  |Δ|=0.0017087   rel=7.278e-04\n",
      "residual_layers.8.output_projection.bias       |Δ|=0.0017023   rel=2.041e-03\n",
      "residual_layers.7.conditioner_projection.bias  |Δ|=0.0017011   rel=5.808e-04\n",
      "residual_layers.7.dilated_conv.bias            |Δ|=0.0017011   rel=3.655e-03\n",
      "residual_layers.1.conditioner_projection.bias  |Δ|=0.0016827   rel=6.659e-04\n",
      "residual_layers.1.dilated_conv.bias            |Δ|=0.0016827   rel=3.178e-03\n",
      "residual_layers.1.output_projection.bias       |Δ|=0.0016809   rel=1.945e-03\n",
      "residual_layers.5.output_projection.bias       |Δ|=0.0016782   rel=2.408e-03\n",
      "residual_layers.6.conditioner_projection.bias  |Δ|=0.001674   rel=9.014e-04\n",
      "residual_layers.6.dilated_conv.bias            |Δ|=0.0016739   rel=3.196e-03\n",
      "cond_upsampler.linear2.weight                  |Δ|=0.0016721   rel=1.831e-03\n",
      "residual_layers.2.dilated_conv.bias            |Δ|=0.0016328   rel=2.992e-03\n",
      "residual_layers.2.conditioner_projection.bias  |Δ|=0.0016327   rel=7.103e-04\n",
      "residual_layers.0.output_projection.bias       |Δ|=0.0016237   rel=2.169e-03\n",
      "residual_layers.7.output_projection.bias       |Δ|=0.0016158   rel=1.956e-03\n",
      "residual_layers.7.diffusion_projection.bias    |Δ|=0.0015888   rel=7.695e-03\n",
      "residual_layers.4.diffusion_projection.bias    |Δ|=0.0015571   rel=1.198e-02\n",
      "residual_layers.4.conditioner_projection.bias  |Δ|=0.0014899   rel=6.740e-04\n",
      "residual_layers.4.dilated_conv.bias            |Δ|=0.0014899   rel=2.877e-03\n",
      "residual_layers.8.diffusion_projection.bias    |Δ|=0.0014206   rel=6.056e-03\n",
      "residual_layers.3.diffusion_projection.bias    |Δ|=0.0013826   rel=8.802e-03\n",
      "residual_layers.8.dilated_conv.bias            |Δ|=0.0013572   rel=2.742e-03\n",
      "residual_layers.8.conditioner_projection.bias  |Δ|=0.0013572   rel=5.739e-04\n",
      "residual_layers.6.diffusion_projection.bias    |Δ|=0.0013235   rel=5.756e-03\n",
      "output_projection.weight                       |Δ|=0.0012868   rel=3.035e-02\n",
      "residual_layers.0.diffusion_projection.bias    |Δ|=0.0012743   rel=5.913e-03\n",
      "residual_layers.9.output_projection.bias       |Δ|=0.0012152   rel=1.573e-03\n",
      "residual_layers.9.diffusion_projection.bias    |Δ|=0.0012103   rel=7.075e-03\n",
      "mlp_state_mapper.hybrid.gate_logits            |Δ|=0.0011761   rel=2.414e-02\n",
      "input_projection.bias                          |Δ|=0.001156   rel=6.269e-04\n",
      "mlp_state_mapper.linear3.bias                  |Δ|=0.0011519   rel=8.026e-04\n",
      "residual_layers.5.diffusion_projection.bias    |Δ|=0.0011223   rel=5.617e-03\n",
      "residual_layers.1.diffusion_projection.bias    |Δ|=0.0010605   rel=6.098e-03\n",
      "skip_projection.bias                           |Δ|=0.0010405   rel=2.045e-03\n",
      "mlp_state_mapper.hybrid.b                      |Δ|=0.0010015   rel=6.881e-05\n",
      "input_projection.weight                        |Δ|=0.00099772   rel=3.204e-04\n",
      "residual_layers.2.diffusion_projection.bias    |Δ|=0.00098256   rel=4.500e-03\n",
      "residual_layers.0.conditioner_projection.weight  |Δ|=0.00093658   rel=4.659e-04\n",
      "mlp_state_mapper.hybrid.W                      |Δ|=0.00089016   rel=1.925e-04\n",
      "residual_layers.5.conditioner_projection.weight  |Δ|=0.00076464   rel=3.243e-04\n",
      "residual_layers.2.conditioner_projection.weight  |Δ|=0.00067906   rel=2.975e-04\n",
      "residual_layers.1.conditioner_projection.weight  |Δ|=0.00062817   rel=3.185e-04\n",
      "mlp_state_mapper.hybrid.log_scale              |Δ|=0.00062649   rel=5.038e-05\n",
      "residual_layers.9.conditioner_projection.weight  |Δ|=0.00060309   rel=2.463e-04\n",
      "residual_layers.7.conditioner_projection.weight  |Δ|=0.00055194   rel=2.069e-04\n",
      "residual_layers.6.conditioner_projection.weight  |Δ|=0.00047163   rel=1.800e-04\n",
      "mlp_state_mapper.preprocess.0.weight           |Δ|=0.00046535   rel=3.051e-04\n",
      "mlp_state_mapper.linear2.bias                  |Δ|=0.00045459   rel=3.414e-03\n",
      "residual_layers.3.conditioner_projection.weight  |Δ|=0.00044844   rel=1.785e-04\n",
      "residual_layers.8.conditioner_projection.weight  |Δ|=0.00039306   rel=1.778e-04\n",
      "mlp_state_mapper.preprocess.0.bias             |Δ|=0.000387   rel=3.420e-04\n",
      "residual_layers.4.conditioner_projection.weight  |Δ|=0.00033707   rel=1.455e-04\n",
      "cond_upsampler.linear3.weight                  |Δ|=0.00030578   rel=5.420e-04\n",
      "output_projection.bias                         |Δ|=0.00017852   rel=6.999e-04\n",
      "\n",
      "[Spectral growth report]\n",
      "residual_layers.0.output_projection.weight     s_max: 2.692 → 3.0104   ×1.118\n",
      "residual_layers.6.dilated_conv.weight          s_max: 0.87488 → 0.90834   ×1.038\n",
      "residual_layers.2.dilated_conv.weight          s_max: 0.93302 → 0.9669   ×1.036\n",
      "output_projection.weight                       s_max: 0.042395 → 0.043664   ×1.030\n",
      "diffusion_embedding.projection1.weight         s_max: 1.7844 → 1.8323   ×1.027\n",
      "residual_layers.1.diffusion_projection.weight  s_max: 0.74387 → 0.76137   ×1.024\n",
      "residual_layers.8.diffusion_projection.weight  s_max: 0.75942 → 0.77528   ×1.021\n",
      "diffusion_embedding.projection2.weight         s_max: 1.7514 → 1.7788   ×1.016\n",
      "residual_layers.4.output_projection.weight     s_max: 2.7176 → 2.7575   ×1.015\n",
      "residual_layers.9.diffusion_projection.weight  s_max: 0.82038 → 0.8285   ×1.010\n",
      "residual_layers.5.diffusion_projection.weight  s_max: 0.79263 → 0.79996   ×1.009\n",
      "residual_layers.6.diffusion_projection.weight  s_max: 0.82902 → 0.83596   ×1.008\n",
      "residual_layers.4.diffusion_projection.weight  s_max: 0.77591 → 0.78058   ×1.006\n",
      "residual_layers.9.dilated_conv.weight          s_max: 0.93927 → 0.94284   ×1.004\n",
      "residual_layers.7.diffusion_projection.weight  s_max: 0.72567 → 0.72821   ×1.003\n",
      "residual_layers.3.output_projection.weight     s_max: 2.7891 → 2.7988   ×1.003\n",
      "residual_layers.0.diffusion_projection.weight  s_max: 0.75264 → 0.7542   ×1.002\n",
      "residual_layers.5.dilated_conv.weight          s_max: 0.99771 → 0.99924   ×1.002\n",
      "skip_projection.weight                         s_max: 2.2498 → 2.2526   ×1.001\n",
      "cond_upsampler.linear2.weight                  s_max: 0.72941 → 0.72998   ×1.001\n",
      "residual_layers.8.dilated_conv.weight          s_max: 1.03 → 1.0307   ×1.001\n",
      "residual_layers.1.output_projection.weight     s_max: 3.2425 → 3.2446   ×1.001\n",
      "cond_upsampler.linear3.weight                  s_max: 0.56418 → 0.56442   ×1.000\n",
      "cond_upsampler.linear1.weight                  s_max: 1.118 → 1.1183   ×1.000\n",
      "mlp_state_mapper.linear2.weight                s_max: 0.70749 → 0.70762   ×1.000\n",
      "residual_layers.6.output_projection.weight     s_max: 3.3221 → 3.3227   ×1.000\n",
      "residual_layers.8.output_projection.weight     s_max: 3.2059 → 3.2063   ×1.000\n",
      "residual_layers.1.conditioner_projection.weight  s_max: 1.9721 → 1.9723   ×1.000\n",
      "residual_layers.0.conditioner_projection.weight  s_max: 2.0101 → 2.0103   ×1.000\n",
      "residual_layers.2.diffusion_projection.weight  s_max: 0.72993 → 0.73   ×1.000\n",
      "residual_layers.0.dilated_conv.weight          s_max: 0.88426 → 0.88433   ×1.000\n",
      "residual_layers.7.conditioner_projection.weight  s_max: 2.6676 → 2.6677   ×1.000\n",
      "residual_layers.7.dilated_conv.weight          s_max: 1.0055 → 1.0055   ×1.000\n",
      "residual_layers.3.conditioner_projection.weight  s_max: 2.5123 → 2.5124   ×1.000\n",
      "residual_layers.4.conditioner_projection.weight  s_max: 2.3159 → 2.3159   ×1.000\n",
      "residual_layers.5.conditioner_projection.weight  s_max: 2.3578 → 2.3578   ×1.000\n",
      "residual_layers.8.conditioner_projection.weight  s_max: 2.2111 → 2.2111   ×1.000\n",
      "mlp_state_mapper.hybrid.W                      s_max: 4.6234 → 4.6234   ×1.000\n",
      "input_projection.weight                        s_max: 3.1144 → 3.1144   ×1.000\n",
      "residual_layers.1.dilated_conv.weight          s_max: 0.9928 → 0.99279   ×1.000\n",
      "residual_layers.7.output_projection.weight     s_max: 2.51 → 2.51   ×1.000\n",
      "residual_layers.9.output_projection.weight     s_max: 3.2255 → 3.2254   ×1.000\n",
      "residual_layers.6.conditioner_projection.weight  s_max: 2.6202 → 2.6201   ×1.000\n",
      "residual_layers.2.output_projection.weight     s_max: 2.8627 → 2.8625   ×1.000\n",
      "residual_layers.9.conditioner_projection.weight  s_max: 2.4489 → 2.4488   ×1.000\n",
      "residual_layers.4.dilated_conv.weight          s_max: 0.94678 → 0.9467   ×1.000\n",
      "residual_layers.2.conditioner_projection.weight  s_max: 2.2824 → 2.2822   ×1.000\n",
      "mlp_state_mapper.preprocess.0.weight           s_max: 1.5251 → 1.5249   ×1.000\n",
      "residual_layers.5.output_projection.weight     s_max: 3.2419 → 3.2413   ×1.000\n",
      "mlp_state_mapper.linear3.weight                s_max: 1.4403 → 1.4351   ×0.996\n",
      "residual_layers.3.diffusion_projection.weight  s_max: 0.80928 → 0.80513   ×0.995\n",
      "residual_layers.3.dilated_conv.weight          s_max: 0.99145 → 0.94894   ×0.957\n",
      "\n",
      "[Activation RMSE report]\n",
      "Activation RMSE per module (same input), top 51:\n",
      "skip_projection                                RMSE=0.182577   n=40000\n",
      "residual_layers.5.diffusion_projection         RMSE=0.161975   n=40000\n",
      "residual_layers.6.diffusion_projection         RMSE=0.158686   n=40000\n",
      "residual_layers.8.diffusion_projection         RMSE=0.153828   n=40000\n",
      "residual_layers.9.diffusion_projection         RMSE=0.153546   n=40000\n",
      "residual_layers.1.diffusion_projection         RMSE=0.145979   n=40000\n",
      "residual_layers.7.diffusion_projection         RMSE=0.140581   n=40000\n",
      "residual_layers.0.diffusion_projection         RMSE=0.136903   n=40000\n",
      "residual_layers.4.diffusion_projection         RMSE=0.130341   n=40000\n",
      "residual_layers.3.diffusion_projection         RMSE=0.128432   n=40000\n",
      "residual_layers.2.diffusion_projection         RMSE=0.106304   n=40000\n",
      "diffusion_embedding.projection2                RMSE=0.0961876   n=320000\n",
      "residual_layers.8.dilated_conv                 RMSE=0.0727236   n=80000\n",
      "residual_layers.5.dilated_conv                 RMSE=0.0718439   n=80000\n",
      "residual_layers.1.dilated_conv                 RMSE=0.0699817   n=80000\n",
      "residual_layers.6.dilated_conv                 RMSE=0.0674359   n=80000\n",
      "residual_layers.4.dilated_conv                 RMSE=0.064941   n=80000\n",
      "residual_layers.3.dilated_conv                 RMSE=0.0635899   n=80000\n",
      "residual_layers.9.dilated_conv                 RMSE=0.0608215   n=80000\n",
      "residual_layers.7.dilated_conv                 RMSE=0.0544579   n=80000\n",
      "residual_layers.0.dilated_conv                 RMSE=0.0500407   n=80000\n",
      "diffusion_embedding.projection1                RMSE=0.0481784   n=320000\n",
      "residual_layers.9.output_projection            RMSE=0.0451615   n=80000\n",
      "residual_layers.2.dilated_conv                 RMSE=0.0436831   n=80000\n",
      "residual_layers.5.output_projection            RMSE=0.0408548   n=80000\n",
      "residual_layers.8.output_projection            RMSE=0.036273   n=80000\n",
      "residual_layers.4.output_projection            RMSE=0.0353948   n=80000\n",
      "residual_layers.6.output_projection            RMSE=0.0352267   n=80000\n",
      "residual_layers.0.output_projection            RMSE=0.0339619   n=80000\n",
      "residual_layers.3.output_projection            RMSE=0.0332254   n=80000\n",
      "residual_layers.7.output_projection            RMSE=0.0307791   n=80000\n",
      "residual_layers.1.output_projection            RMSE=0.0294038   n=80000\n",
      "output_projection                              RMSE=0.0263508   n=5000\n",
      "residual_layers.2.output_projection            RMSE=0.0168037   n=80000\n",
      "cond_upsampler.linear2                         RMSE=0.00259916   n=10000\n",
      "cond_upsampler.linear1                         RMSE=0.00163797   n=100000\n",
      "mlp_state_mapper.linear2                       RMSE=0.00161549   n=20000\n",
      "mlp_state_mapper.linear3                       RMSE=0.00110082   n=100000\n",
      "cond_upsampler.linear3                         RMSE=0.00100974   n=5000\n",
      "residual_layers.3.conditioner_projection       RMSE=0.000615656   n=80000\n",
      "residual_layers.7.conditioner_projection       RMSE=0.00060158   n=80000\n",
      "residual_layers.5.conditioner_projection       RMSE=0.000597599   n=80000\n",
      "residual_layers.1.conditioner_projection       RMSE=0.000564183   n=80000\n",
      "residual_layers.0.conditioner_projection       RMSE=0.000542225   n=80000\n",
      "residual_layers.9.conditioner_projection       RMSE=0.000522969   n=80000\n",
      "residual_layers.4.conditioner_projection       RMSE=0.000507659   n=80000\n",
      "residual_layers.6.conditioner_projection       RMSE=0.000491594   n=80000\n",
      "residual_layers.2.conditioner_projection       RMSE=0.000446909   n=80000\n",
      "residual_layers.8.conditioner_projection       RMSE=0.000426627   n=80000\n",
      "input_projection                               RMSE=0.000409404   n=40000\n",
      "mlp_state_mapper.preprocess.0                  RMSE=0.000230167   n=20000\n",
      "\n",
      "[Output drift + loss change]\n",
      "Output drift: MSE=0.000694363  MAE=0.0263508\n",
      "Surrogate loss: pre=0.00633726  post=0.0072109\n",
      "\n",
      "[Per-eff_times-bin output drift]\n",
      "Per-eff_times-bin output MSE: [None, None, None, None, None, 3.153e-08]\n",
      "\n",
      "[Input gradient norm (optional)]\n",
      "good   input-grad: mean=0.0031495  median=0.00251203  max=0.111659\n",
      "bad    input-grad: mean=0.00346457  median=0.00282339  max=0.110121\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Good vs Bad checkpoint comparison (non-mutating, shape-safe)\n",
    "# Expects:\n",
    "#   good_scoreModel  # \"before spike\"   (already loaded)\n",
    "#   bad_scoreModel   # \"after spike\"    (already loaded)\n",
    "#\n",
    "# You provide one sample dict with the SAME preprocessing you use at eval:\n",
    "# sample = {\n",
    "#   \"inputs\":      Tensor [B,1,D] or [B,D] or [D],\n",
    "#   \"times\":       Tensor [B] or scalar,\n",
    "#   \"conditioner\": Tensor [B,1,D] or [B,D] or [D],\n",
    "#   \"eff_times\":   Tensor [B,1,1] or [B] or scalar,\n",
    "# }\n",
    "#\n",
    "# What this reports:\n",
    "#   1) Weight deltas + cosine similarity\n",
    "#   2) Spectral growth (top singular values) per layer\n",
    "#   3) Activation RMSE per key module (same input)\n",
    "#   4) Output drift (MSE/MAE) and surrogate loss change (same input)\n",
    "#   5) Per-eff_times-bin output drift (localizes regime)\n",
    "#   6) Input-gradient norm (optional) to detect curvature/exploding sensitivity\n",
    "# ============================================================\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# Shape handling (no padding/trim)\n",
    "# ------------------------------\n",
    "def _to_dev_dtype(x, ref_param):\n",
    "    if x.dtype.is_floating_point:\n",
    "        return x.to(ref_param.device, ref_param.dtype)\n",
    "    else:\n",
    "        return x.to(ref_param.device)\n",
    "\n",
    "def standardize_sample(sample: dict, model: nn.Module) -> dict:\n",
    "    p = next(model.parameters())\n",
    "\n",
    "    # inputs -> [B,1,D]\n",
    "    x = _to_dev_dtype(sample[\"inputs\"], p)\n",
    "    if x.ndim == 1:            # [D]\n",
    "        x = x.unsqueeze(0).unsqueeze(0)               # [1,1,D]\n",
    "    elif x.ndim == 2:          # [B,D]\n",
    "        x = x.unsqueeze(1)                             # [B,1,D]\n",
    "    elif x.ndim == 3:\n",
    "        assert x.size(1) == 1, \"inputs second dim must be 1\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"inputs bad shape {tuple(x.shape)}\")\n",
    "\n",
    "    B = x.size(0)\n",
    "\n",
    "    # conditioner -> [B,1,D] (match B; keep D as-is)\n",
    "    c = _to_dev_dtype(sample[\"conditioner\"], p)\n",
    "    if c.ndim == 1:            # [D]\n",
    "        c = c.unsqueeze(0).unsqueeze(1)               # [1,1,D]\n",
    "    elif c.ndim == 2:          # [B,D]\n",
    "        c = c.unsqueeze(1)                             # [B,1,D]\n",
    "    elif c.ndim == 3:\n",
    "        assert c.size(1) == 1, \"conditioner second dim must be 1\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"conditioner bad shape {tuple(c.shape)}\")\n",
    "    if c.size(0) == 1 and B > 1:\n",
    "        c = c.expand(B, -1, -1)\n",
    "    assert c.size(-1) == x.size(-1), f\"D mismatch: inputs D={x.size(-1)} vs cond D={c.size(-1)}\"\n",
    "\n",
    "    # times -> [B]\n",
    "    t = sample[\"times\"].to(p.device).view(-1)\n",
    "    if t.numel() == 1: t = t.expand(B)\n",
    "\n",
    "    # eff_times -> [B,1,1]\n",
    "    et = _to_dev_dtype(sample[\"eff_times\"], p).view(-1)\n",
    "    if et.numel() == 1: et = et.expand(B)\n",
    "    et = et.view(B, 1, 1)\n",
    "\n",
    "    return {\"inputs\": x, \"times\": t, \"conditioner\": c, \"eff_times\": et}\n",
    "\n",
    "# ------------------------------\n",
    "# Non-mutating hook helpers\n",
    "# ------------------------------\n",
    "def clear_all_hooks(model: nn.Module) -> None:\n",
    "    for m in model.modules():\n",
    "        m._forward_hooks.clear()\n",
    "        m._forward_pre_hooks.clear()\n",
    "        m._backward_hooks.clear()\n",
    "\n",
    "# ------------------------------\n",
    "# Reports: weights and spectra\n",
    "# ------------------------------\n",
    "def flat_vec(model: nn.Module, device=\"cpu\", dtype=torch.float64) -> torch.Tensor:\n",
    "    parts = [p.detach().to(device=device, dtype=dtype).flatten()\n",
    "             for p in model.parameters() if p.requires_grad]\n",
    "    return torch.cat(parts) if parts else torch.tensor([], dtype=dtype, device=device)\n",
    "\n",
    "def weight_diff_report(model0: nn.Module, model1: nn.Module, top=25) -> None:\n",
    "    pb = dict(model1.named_parameters())\n",
    "    rows, tot_sq = [], 0.0\n",
    "    for n, pa in model0.named_parameters():\n",
    "        if not pa.requires_grad or n not in pb: continue\n",
    "        d = (pb[n].detach() - pa.detach()).flatten()\n",
    "        dn = d.norm().item()\n",
    "        wn = pa.detach().flatten().norm().item()\n",
    "        rows.append((n, dn, dn/(wn+1e-12)))\n",
    "        tot_sq += dn*dn\n",
    "    rows.sort(key=lambda x: x[1], reverse=True)\n",
    "    v0 = flat_vec(model0); v1 = flat_vec(model1)\n",
    "    cos = F.cosine_similarity(v0.unsqueeze(0), v1.unsqueeze(0), dim=1).item() if v0.numel() and v1.numel() else float(\"nan\")\n",
    "    print(f\"||Δθ|| = {math.sqrt(tot_sq):.6g}\")\n",
    "    print(f\"cosine(flat_weights_pre, flat_weights_post) = {cos:.6f}\")\n",
    "    for n, d, r in rows[:top]:\n",
    "        print(f\"{n:45s}  |Δ|={d:.5g}   rel={r:.3e}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def _top_singular_value(W: torch.Tensor, iters=15) -> float:\n",
    "    if W.ndim < 2: return float(\"nan\")\n",
    "    M = W.reshape(W.size(0), -1)\n",
    "    u = F.normalize(torch.randn(M.size(0), device=M.device), dim=0)\n",
    "    for _ in range(iters):\n",
    "        v = F.normalize(M.t().mv(u), dim=0)\n",
    "        u = F.normalize(M.mv(v), dim=0)\n",
    "    return u.dot(M.mv(v)).abs().item()\n",
    "\n",
    "def spectral_report(model0: nn.Module, model1: nn.Module, top=12) -> None:\n",
    "    pb = dict(model1.named_parameters())\n",
    "    rows = []\n",
    "    for n, p0 in model0.named_parameters():\n",
    "        if n not in pb or p0.ndim < 2: continue\n",
    "        s0 = _top_singular_value(p0.detach())\n",
    "        s1 = _top_singular_value(pb[n].detach())\n",
    "        rows.append((n, s0, s1, s1/(s0+1e-12)))\n",
    "    rows.sort(key=lambda x: x[3], reverse=True)\n",
    "    for n, s0, s1, r in rows[:top]:\n",
    "        print(f\"{n:45s}  s_max: {s0:.5g} → {s1:.5g}   ×{r:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Activation diffs (same input)\n",
    "# ------------------------------\n",
    "def list_capture_modules(model: nn.Module) -> list[str]:\n",
    "    names = []\n",
    "    for name, mod in model.named_modules():\n",
    "        if isinstance(mod, (nn.Conv1d, nn.Linear)):\n",
    "            names.append(name)\n",
    "    # Deduplicate and keep stable order\n",
    "    seen, out = set(), []\n",
    "    for n in names:\n",
    "        if n not in seen:\n",
    "            out.append(n); seen.add(n)\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def capture_activations(model: nn.Module, sample: dict, names: list[str]) -> dict[str, torch.Tensor]:\n",
    "    bufs = {}\n",
    "    hs = []\n",
    "    for name, mod in model.named_modules():\n",
    "        if name in names:\n",
    "            def make_hook(nm):\n",
    "                def _h(m, inp, out):\n",
    "                    if isinstance(out, (tuple, list)):\n",
    "                        out = out[0]\n",
    "                    bufs[nm] = out.detach().float().flatten()\n",
    "                return _h\n",
    "            hs.append(mod.register_forward_hook(make_hook(name)))\n",
    "    s = standardize_sample(sample, model)\n",
    "    _ = model(s[\"inputs\"], s[\"times\"], s[\"conditioner\"], s[\"eff_times\"])\n",
    "    for h in hs: h.remove()\n",
    "    return bufs\n",
    "\n",
    "def activation_diff_report(good: nn.Module, bad: nn.Module, sample: dict, top=25) -> None:\n",
    "    names = list_capture_modules(good)\n",
    "    a0 = capture_activations(good, sample, names)\n",
    "    a1 = capture_activations(bad,  sample, names)\n",
    "    rows = []\n",
    "    for k in names:\n",
    "        if k in a0 and k in a1:\n",
    "            d = (a1[k] - a0[k])\n",
    "            rmse = d.pow(2).mean().sqrt().item()\n",
    "            rows.append((k, rmse, a0[k].numel()))\n",
    "    rows.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Activation RMSE per module (same input), top {min(top,len(rows))}:\")\n",
    "    for k, r, n in rows[:top]:\n",
    "        print(f\"{k:45s}  RMSE={r:.6g}   n={n}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Output drift + loss change\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def output_and_loss(model: nn.Module, sample: dict, loss_fn=None):\n",
    "    s = standardize_sample(sample, model)\n",
    "    y = model(s[\"inputs\"], s[\"times\"], s[\"conditioner\"], s[\"eff_times\"])\n",
    "    beta_tau = torch.exp(-0.5 * s[\"eff_times\"])\n",
    "    sigma2_tau = (1. - torch.exp(-s[\"eff_times\"]))\n",
    "    # Network tries to learn the posterior mean\n",
    "    y = (y + s[\"inputs\"] / sigma2_tau) * (sigma2_tau / beta_tau)  # This gives us the network D_theta\n",
    "    if loss_fn is None:\n",
    "        loss = F.mse_loss(y, s[\"inputs\"])\n",
    "    else:\n",
    "        loss = loss_fn(y, s[\"inputs\"])\n",
    "    return y, loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def output_drift_and_loss_change(good: nn.Module, bad: nn.Module, sample: dict, loss_fn=None):\n",
    "    y0, l0 = output_and_loss(good, sample, loss_fn)\n",
    "    y1, l1 = output_and_loss(bad,  sample, loss_fn)\n",
    "    mse = F.mse_loss(y1, y0).item()\n",
    "    mae = (y1 - y0).abs().mean().item()\n",
    "    print(f\"Output drift: MSE={mse:.6g}  MAE={mae:.6g}\")\n",
    "    print(f\"Surrogate loss: pre={l0:.6g}  post={l1:.6g}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Per-eff_times-bin drift\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def drift_by_tbin(good: nn.Module, bad: nn.Module, sample: dict, nbins=6):\n",
    "    sg = standardize_sample(sample, good)\n",
    "    sb = standardize_sample(sample, bad)\n",
    "    et = sg[\"eff_times\"].view(sg[\"eff_times\"].size(0))  # [B]\n",
    "    q = torch.quantile(et, torch.linspace(0,1,nbins+1, device=et.device))\n",
    "    q[0] -= 1e-6; q[-1] += 1e-6\n",
    "    vals = []\n",
    "    for i in range(nbins):\n",
    "        m = (et >= q[i]) & (et < q[i+1])\n",
    "        if m.any():\n",
    "            y0 = good(sg[\"inputs\"][m], sg[\"times\"][m], sg[\"conditioner\"][m], sg[\"eff_times\"][m])\n",
    "            y1 = bad (sb[\"inputs\"][m], sb[\"times\"][m], sb[\"conditioner\"][m], sb[\"eff_times\"][m])\n",
    "            vals.append(F.mse_loss(y1, y0).item())\n",
    "        else:\n",
    "            vals.append(float('nan'))\n",
    "    print(\"Per-eff_times-bin output MSE:\", [None if math.isnan(v) else float(f\"{v:.4g}\") for v in vals])\n",
    "\n",
    "# ------------------------------\n",
    "# Input gradient norm (optional)\n",
    "# ------------------------------\n",
    "def input_grad_norm(model: nn.Module, sample: dict, loss_fn=None):\n",
    "    s = standardize_sample(sample, model)\n",
    "    x = s[\"inputs\"].detach().clone().requires_grad_(True)\n",
    "    y = model(x, s[\"times\"], s[\"conditioner\"], s[\"eff_times\"])\n",
    "    beta_tau = torch.exp(-0.5 * s[\"eff_times\"])\n",
    "    sigma2_tau = (1. - torch.exp(-s[\"eff_times\"]))\n",
    "    # Network tries to learn the posterior mean\n",
    "    y = (y + s[\"inputs\"] / sigma2_tau) * (sigma2_tau / beta_tau)  # This gives us the network D_theta\n",
    "    if loss_fn is None:\n",
    "        loss = F.mse_loss(y, x)\n",
    "    else:\n",
    "        loss = loss_fn(y, x)\n",
    "    loss.backward()\n",
    "    g = x.grad.detach().flatten(1).norm(dim=1)  # [B]\n",
    "    return g.mean().item(), g.median().item(), g.max().item()\n",
    "\n",
    "# ------------------------------\n",
    "# Entry point\n",
    "# ------------------------------\n",
    "def compare_models(good_scoreModel: nn.Module, bad_scoreModel: nn.Module, sample: dict, loss_fn=None):\n",
    "    good_scoreModel.eval(); bad_scoreModel.eval()\n",
    "    clear_all_hooks(good_scoreModel); clear_all_hooks(bad_scoreModel)\n",
    "\n",
    "    print(\"\\n[Weight delta report]\")\n",
    "    weight_diff_report(good_scoreModel, bad_scoreModel, top=300)\n",
    "\n",
    "    print(\"\\n[Spectral growth report]\")\n",
    "    spectral_report(good_scoreModel, bad_scoreModel, top=300)\n",
    "\n",
    "    print(\"\\n[Activation RMSE report]\")\n",
    "    activation_diff_report(good_scoreModel, bad_scoreModel, sample, top=300)\n",
    "\n",
    "    print(\"\\n[Output drift + loss change]\")\n",
    "    output_drift_and_loss_change(good_scoreModel, bad_scoreModel, sample, loss_fn=loss_fn)\n",
    "\n",
    "    print(\"\\n[Per-eff_times-bin output drift]\")\n",
    "    drift_by_tbin(good_scoreModel, bad_scoreModel, sample, nbins=6)\n",
    "\n",
    "    print(\"\\n[Input gradient norm (optional)]\")\n",
    "    m, med, mx = input_grad_norm(good_scoreModel, sample, loss_fn=loss_fn)\n",
    "    print(f\"good   input-grad: mean={m:.6g}  median={med:.6g}  max={mx:.6g}\")\n",
    "    m, med, mx = input_grad_norm(bad_scoreModel, sample, loss_fn=loss_fn)\n",
    "    print(f\"bad    input-grad: mean={m:.6g}  median={med:.6g}  max={mx:.6g}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Usage:\n",
    "# ------------------------------\n",
    "# sample = {\n",
    "#     \"inputs\":      torch.randn(2,1,1),   # D=1 throughout\n",
    "#     \"times\":       torch.tensor([1.0, 1.0]),\n",
    "#     \"conditioner\": torch.randn(2,1,1),\n",
    "#     \"eff_times\":   torch.tensor([10.0, 10.0]),\n",
    "# }\n",
    "compare_models(good_scoreModel, bad_scoreModel, sample, loss_fn=None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
