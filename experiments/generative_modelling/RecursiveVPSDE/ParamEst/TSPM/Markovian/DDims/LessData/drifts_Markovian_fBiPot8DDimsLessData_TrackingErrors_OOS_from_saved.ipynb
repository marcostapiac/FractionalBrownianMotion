{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from configs import project_config\n",
    "from configs.RecursiveVPSDE.Markovian_fBiPotDDims.recursive_Markovian_PostMeanScore_fBiPot8Dims_T256_H05_tl_110data_StbleTgt  import get_config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rmse_ignore_nans(y_true, y_pred):\n",
    "    return np.nanmean((y_true-y_pred)**2)\n",
    "\n",
    "def plot_ewma_losses(epochs, losses):\n",
    "    start_idx = 00\n",
    "    def compute_ema(loss_tensor, beta):\n",
    "        ema_values = np.zeros_like(loss_tensor)  # Initialize EMA tensor\n",
    "        ema_values[0] = loss_tensor[0]  # First value stays the same\n",
    "\n",
    "        for i in range(1, len(loss_tensor)):\n",
    "            ema_values[i] = beta * ema_values[i - 1] + (1 - beta) * loss_tensor[i]\n",
    "\n",
    "        return ema_values\n",
    "    # Define EMA decay rates\n",
    "    beta_short = 0.9   # Short-term trend (reacts quickly)\n",
    "    beta_long = 0.99   # Long-term trend (smoother)\n",
    "\n",
    "    # Compute EMAs\n",
    "    short_term_ema = compute_ema(losses, beta_short)\n",
    "    long_term_ema = compute_ema(losses, beta_long)\n",
    "    plt.scatter(epochs[start_idx:], (short_term_ema[start_idx:]),s=2, label=\"Short Term Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.scatter(epochs[start_idx:], (long_term_ema[start_idx:]),s=2, label=\"Long Term Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "deltaT = config.deltaT\n",
    "assert config.feat_thresh == 1.\n",
    "root_dir =\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/\"\n",
    "print(config.loss_factor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/marcos/GitHubRepos/FractionalBrownianMotion/src/generative_modelling/training_losses/trained_rec_ST_1000FTh_PM_MLP_2LFac_NSTgtNFMReg_fBiPot_8DDims_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_025a_-20b_00c_MLP_H4_CUp20_tl110_loss'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscoreNet_trained_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/trained_models/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/training_losses/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_loss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      2\u001B[0m         losses \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39marray(pickle\u001B[38;5;241m.\u001B[39mload(f))\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m))\n\u001B[1;32m      3\u001B[0m Nepochs_losses \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(losses\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/GitHubRepos/FractionalBrownianMotion/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/marcos/GitHubRepos/FractionalBrownianMotion/src/generative_modelling/training_losses/trained_rec_ST_1000FTh_PM_MLP_2LFac_NSTgtNFMReg_fBiPot_8DDims_VPSDE_T256_Ndiff10000_Tdiff1000e+00_DiffEmbSz64_ResLay10_ResChan8_DiffHdnSz64_TrueHybd_TrueWghts_t00_dT3906e-03_025a_-20b_00c_MLP_H4_CUp20_tl110_loss'"
     ]
    }
   ],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss\", 'rb') as f:\n",
    "        losses = (np.array(pickle.load(f)).astype(float))\n",
    "Nepochs_losses = np.arange(losses.shape[0])\n",
    "plt.scatter(Nepochs_losses,  losses, s=10)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "Nepochs_losses[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_ewma_losses(epochs=Nepochs_losses, losses=losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss_LR\", 'rb') as f:\n",
    "        LRs = (np.array(pickle.load(f)).astype(float))\n",
    "start = 0#935\n",
    "end = -1#935+152\n",
    "plt.scatter(Nepochs_losses[start:end],  LRs[start:end], s=10, label=\"Learning Rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(LRs[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss\", 'rb') as f:\n",
    "        losses = np.array(pickle.load(f)).astype(float)\n",
    "Nepochs_losses = np.arange(losses.shape[0])\n",
    "print(Nepochs_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/8DDimsLessData/\"\n",
    "mses=[]\n",
    "for file in os.listdir(root_score_dir):\n",
    "    if \"MSE\" in file:\n",
    "        mses.append(pd.read_parquet(root_score_dir+file, engine=\"fastparquet\"))\n",
    "mses = pd.concat(mses).reset_index(drop=False).rename({\"index\":\"epoch\"}, axis=1).sort_values(\"epoch\").reset_index(drop=True)\n",
    "Nepochs_track = mses[\"epoch\"].values.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "common_epochs = np.intersect1d(Nepochs_losses, Nepochs_track)\n",
    "common_epochs = np.intersect1d(common_epochs, np.arange(100, 8000))\n",
    "start_idx = 0\n",
    "common_epochs = common_epochs[start_idx:]\n",
    "losses_idx = np.unique([np.argwhere(c == Nepochs_losses)[0,0] for c in common_epochs])\n",
    "track_idx = np.unique([np.argwhere(c == Nepochs_track)[0,0] for c in common_epochs])\n",
    "red_losses = losses[losses_idx]\n",
    "print(track_idx.shape, common_epochs.shape)\n",
    "#track_rmses = np.array(list(drift_track_rmses.values()))[track_idx]\n",
    "track_rmses = mses.iloc[track_idx,:].mse.values.flatten()\n",
    "print(track_rmses.shape, common_epochs.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "toSave = False\n",
    "fig, ax = plt.subplots(figsize=(14,9))\n",
    "max_idx = np.argwhere(common_epochs <=40000).flatten()[-1]\n",
    "sc1 = ax.scatter(common_epochs[:max_idx+1], red_losses[:max_idx+1], s=50, label=\"Training Loss\")\n",
    "ax2 = ax.twinx()\n",
    "sc2 = ax2.scatter(common_epochs[:max_idx+1], track_rmses[:max_idx+1], s=50, color=\"red\",label=\"$E^{(\\mu)}_{256}$\")\n",
    "ax.set_xlabel(\"Training Epochs\", fontsize=38)\n",
    "ax.set_title(r\"Losses for $\\mu_{4}$ with $D=8, c=20$\", fontsize=40)\n",
    "ax.tick_params(axis=\"both\",labelsize=38)\n",
    "ax2.tick_params(axis=\"both\",labelsize=38)\n",
    "plt.tight_layout()\n",
    "handles = [sc1, sc2]\n",
    "labels = [h.get_label() for h in handles]\n",
    "# Add a single legend on ax1\n",
    "ax.legend(handles, labels, fontsize=30, markerscale=2)\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(mses.iloc[np.argmin(mses.mse), 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
