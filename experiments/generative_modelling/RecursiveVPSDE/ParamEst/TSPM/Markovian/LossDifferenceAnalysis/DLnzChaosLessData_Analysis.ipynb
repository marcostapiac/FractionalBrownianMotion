{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from src.generative_modelling.models.TimeDependentScoreNetworks.ClassConditionalMarkovianTSPostMeanScoreMatching import \\\n",
    "    ConditionalMarkovianTSPostMeanScoreMatching\n",
    "from utils.drift_evaluation_functions import experiment_MLP_DDims_drifts\n",
    "from configs.RecursiveVPSDE.Markovian_8DLorenz.recursive_Markovian_PostMeanScore_8DLorenz_Chaos_T256_H05_tl_110data_StbleTgt import get_config as get_8dlnz_config\n",
    "from configs.RecursiveVPSDE.Markovian_12DLorenz.recursive_Markovian_PostMeanScore_12DLorenz_Chaos_T256_H05_tl_110data_StbleTgt import get_config as get_12dlnz_config\n",
    "from configs.RecursiveVPSDE.Markovian_20DLorenz.recursive_Markovian_PostMeanScore_20DLorenz_Chaos_T256_H05_tl_110data_StbleTgt import get_config as get_20dlnz_config\n",
    "from configs.RecursiveVPSDE.Markovian_40DLorenz.recursive_Markovian_PostMeanScore_40DLorenz_Chaos_T256_H05_tl_110data_StbleTgt import get_config as get_40dlnz_config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.drift_evaluation_functions import multivar_score_based_MLP_drift_OOS\n",
    "from src.generative_modelling.models.ClassVPSDEDiffusion import VPSDEDiffusion\n",
    "from configs import project_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _get_device(device_str: str | None = None):\n",
    "    if device_str is not None:\n",
    "        return torch.device(device_str)\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def true_drifts(device_id, config, state):\n",
    "    true_drifts = np.zeros_like(state)\n",
    "    for i in range(config.ndims):\n",
    "        true_drifts[:, i] = (state[:, (i + 1) % config.ndims] - state[:, i - 2]) * state[:, i - 1] - state[:,i] * config.forcing_const\n",
    "    return torch.tensor(true_drifts[:, np.newaxis, :], device=device_id, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lnz_8d_config = get_8dlnz_config()\n",
    "lnz_12d_config = get_12dlnz_config()\n",
    "lnz_20d_config = get_20dlnz_config()\n",
    "lnz_40d_config = get_40dlnz_config()\n",
    "device_id = _get_device()\n",
    "assert lnz_8d_config.feat_thresh == lnz_12d_config.feat_thresh == lnz_20d_config.feat_thresh == lnz_40d_config.feat_thresh\n",
    "num_paths = 1024 if lnz_8d_config.feat_thresh == 1. else 10240\n",
    "assert num_paths == 1024\n",
    "root_dir =\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_paths(config, device_id, good):\n",
    "    diffusion = VPSDEDiffusion(beta_max=config.beta_max, beta_min=config.beta_min)\n",
    "    num_diff_times = 1\n",
    "    rmse_quantile_nums = 1\n",
    "    num_paths = 100\n",
    "    num_time_steps = config.ts_length\n",
    "    deltaT = config.deltaT\n",
    "    all_true_states = np.zeros(shape=(rmse_quantile_nums, num_paths, 1 + num_time_steps, config.ndims))\n",
    "    all_global_states = np.zeros(shape=(rmse_quantile_nums, num_paths, 1 + num_time_steps, config.ndims))\n",
    "    all_local_states = np.zeros(shape=(rmse_quantile_nums, num_paths, 1 + num_time_steps, config.ndims))\n",
    "    for quant_idx in tqdm(range(rmse_quantile_nums)):\n",
    "        good.eval()\n",
    "        initial_state = np.repeat(np.atleast_2d(config.initState)[np.newaxis, :], num_paths, axis=0)\n",
    "        assert (initial_state.shape == (num_paths, 1, config.ndims))\n",
    "\n",
    "        true_states = np.zeros(shape=(num_paths, 1 + num_time_steps, config.ndims))\n",
    "        global_states = np.zeros(shape=(num_paths, 1 + num_time_steps, config.ndims))\n",
    "        local_states = np.zeros(shape=(num_paths, 1 + num_time_steps, config.ndims))\n",
    "\n",
    "        # Initialise the \"true paths\"\n",
    "        true_states[:, [0], :] = initial_state + 0.00001 * np.random.randn(*initial_state.shape)\n",
    "        # Initialise the \"global score-based drift paths\"\n",
    "        global_states[:, [0], :] = true_states[:, [0], :]\n",
    "        local_states[:, [0], :] = true_states[:, [0],\n",
    "                                  :]  # np.repeat(initial_state[np.newaxis, :], num_diff_times, axis=0)\n",
    "\n",
    "        # Euler-Maruyama Scheme for Tracking Errors\n",
    "        for i in range(1, num_time_steps + 1):\n",
    "            eps = np.random.randn(num_paths, 1, config.ndims) * np.sqrt(deltaT) * config.diffusion\n",
    "\n",
    "            assert (eps.shape == (num_paths, 1, config.ndims))\n",
    "            true_mean = true_drifts(state=true_states[:, i - 1, :], device_id=device_id,config=config).cpu().numpy()\n",
    "            denom = 1.\n",
    "            global_mean = multivar_score_based_MLP_drift_OOS(score_model=good,\n",
    "                                                             num_diff_times=num_diff_times,\n",
    "                                                             diffusion=diffusion,\n",
    "                                                             num_paths=num_paths,\n",
    "                                                             ts_step=deltaT, config=config,\n",
    "                                                             device=device_id,\n",
    "                                                             prev=global_states[:, i - 1, :])\n",
    "\n",
    "            true_states[:, [i], :] = (true_states[:, [i - 1], :] \\\n",
    "                                      + true_mean * deltaT \\\n",
    "                                      + eps) / denom\n",
    "            global_states[:, [i], :] = (global_states[:, [i - 1], :] + global_mean * deltaT + eps) / denom\n",
    "\n",
    "        all_true_states[quant_idx, :, :, :] = true_states\n",
    "        all_global_states[quant_idx, :, :, :] = global_states\n",
    "    return all_true_states, all_local_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_epoch(config, type):\n",
    "    model_dir = \"/\".join(config.scoreNet_trained_path.split(\"/\")[:-1]) + \"/\"\n",
    "    for file in os.listdir(model_dir):\n",
    "        if config.scoreNet_trained_path in os.path.join(model_dir, file) and f\"{type}\" in file:\n",
    "            best_epoch = int(file.split(f\"{type}NEp\")[-1])\n",
    "    return best_epoch\n",
    "\n",
    "def get_best_track_file(root_score_dir, ts_type, best_epoch_track):\n",
    "    for file in os.listdir(root_score_dir):\n",
    "        if (\"_\"+str(best_epoch_track)+\"Nep\") in file and \"true\" in file and ts_type in file and \"1000FTh\" in file and \"125FConst\" in file:\n",
    "            with open(root_score_dir+file, 'rb') as f:\n",
    "                buf = io.BytesIO(f.read())  # hydrates once, sequentially\n",
    "            true_file = np.load(root_score_dir+file, allow_pickle=True)\n",
    "        elif (\"_\"+str(best_epoch_track)+\"Nep\") in file and \"global\" in file and ts_type in file and \"1000FTh\" in file and \"125FConst\" in file:\n",
    "            with open(root_score_dir+file, 'rb') as f:\n",
    "                buf = io.BytesIO(f.read())  # hydrates once, sequentially\n",
    "            global_file = np.load(root_score_dir+file, allow_pickle=True)\n",
    "    print(ts_type)\n",
    "    return true_file, global_file\n",
    "\n",
    "def get_best_eval_exp_file(config, root_score_dir, ts_type):\n",
    "    best_epoch_eval = get_best_epoch(config=config,type=\"EE\")\n",
    "    for file in os.listdir(root_score_dir):\n",
    "        if (\"_\"+str(best_epoch_eval)+\"Nep\") in file and \"MSE\" in file and ts_type in file and \"1000FTh\" in file and \"125FConst\" in file:\n",
    "            print(f\"Starting {file}\\n\")\n",
    "            with open(root_score_dir+file, 'rb') as f:\n",
    "                buf = io.BytesIO(f.read())  # hydrates once, sequentially\n",
    "            print(f\"Starting {file}\\n\")\n",
    "            mse = pd.read_parquet(root_score_dir+file, engine=\"fastparquet\")\n",
    "    return mse\n",
    "@torch.no_grad()\n",
    "def IID_NW_multivar_estimator_gpu(\n",
    "    prevPath_observations: torch.Tensor,  # (N,n,d) float32 CUDA\n",
    "    path_incs: torch.Tensor,              # (N,n,d) float32 CUDA\n",
    "    inv_H: torch.Tensor,                  # (d,) (diag) or (d,d) float32 CUDA\n",
    "    norm_const: float,                    # same meaning as your CPU code\n",
    "    x: torch.Tensor,                      # (M,d) float32 CUDA\n",
    "    t1: float,\n",
    "    t0: float,\n",
    "    truncate: bool = True,\n",
    "    M_tile: int = 32,                     # micro-batch states\n",
    "    Nn_tile: int | None = 512_000,        # micro-batch samples (None => full)\n",
    "    stable: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns: (M,d) float32 CUDA tensor (keeps all heavy ops on LongerTimes_GPU).\n",
    "    Matches your scaling:\n",
    "      denom = sum(w)/(N*n)\n",
    "      numer = (sum(w * incs)/N) * (t1 - t0)\n",
    "    \"\"\"\n",
    "    #assert prevPath_observations.is_cuda and path_incs.is_cuda and x.is_cuda\n",
    "    assert prevPath_observations.dtype == torch.float32\n",
    "    assert path_incs.dtype == torch.float32\n",
    "    assert x.dtype == torch.float32\n",
    "\n",
    "    N, n, d = prevPath_observations.shape\n",
    "    Nn = N * n\n",
    "    if Nn_tile is None or Nn_tile > Nn:\n",
    "        Nn_tile = Nn\n",
    "\n",
    "    # Flatten once\n",
    "    mu = prevPath_observations.reshape(Nn, d).contiguous()  # (Nn,d)\n",
    "    dX = path_incs.reshape(Nn, d).contiguous()              # (Nn,d)\n",
    "\n",
    "    # Diagonal vs full inv_H\n",
    "    diag = (inv_H.ndim == 1)\n",
    "    if diag:\n",
    "        A = inv_H                                           # (d,)\n",
    "        muAh = mu * A                                       # (Nn,d)\n",
    "        mu_quad = (mu * muAh).sum(-1)                       # (Nn,)\n",
    "        def xAh(X): return X * A\n",
    "    else:\n",
    "        A = inv_H                                           # (d,d)\n",
    "        muAh = mu @ A                                       # (Nn,d)\n",
    "        mu_quad = (mu * muAh).sum(-1)                       # (Nn,)\n",
    "        # Sanity: PD\n",
    "        sign, _ = torch.linalg.slogdet(A)\n",
    "        if sign.item() <= 0:\n",
    "            raise ValueError(\"inv_H must be positive definite.\")\n",
    "\n",
    "    # Use log(norm_const) directly to match your CPU estimator\n",
    "    log_norm_const = float(np.log(norm_const))\n",
    "\n",
    "    M = x.size(0)\n",
    "    # Accumulate in float64 for stability\n",
    "    denom = torch.zeros(M,     dtype=torch.float64, device=x.device)\n",
    "    numer = torch.zeros(M, d,  dtype=torch.float64, device=x.device)\n",
    "\n",
    "    for m0 in range(0, M, M_tile):\n",
    "        X = x[m0:m0 + M_tile]                     # (mb,d)\n",
    "        XAh = xAh(X)                               # (mb,d)\n",
    "        X_quad = (X * XAh).sum(-1)                 # (mb,)\n",
    "\n",
    "        denom_tile = torch.zeros(X.size(0),    dtype=torch.float64, device=x.device)\n",
    "        numer_tile = torch.zeros(X.size(0), d, dtype=torch.float64, device=x.device)\n",
    "\n",
    "        if stable:\n",
    "            lse_max = torch.full((X.size(0),), -torch.inf, dtype=torch.float32, device=x.device)\n",
    "            # First pass: find max exponent per state (over all Nn tiles)\n",
    "            for i0 in range(0, Nn, Nn_tile):\n",
    "                muq_i  = mu_quad[i0:i0 + Nn_tile]             # (bn,)\n",
    "                muAh_i = muAh[i0:i0 + Nn_tile]                # (bn,d)\n",
    "                cross  = muAh_i @ X.t()                       # (bn,mb)\n",
    "                expo   = log_norm_const - 0.5 * (muq_i[:, None] + X_quad[None, :] - 2.0 * cross)\n",
    "                lse_max = torch.maximum(lse_max, expo.max(dim=0).values)\n",
    "\n",
    "        # Second pass: accumulate with stabilization (or plain)\n",
    "        for i0 in range(0, Nn, Nn_tile):\n",
    "            muAh_i = muAh[i0:i0 + Nn_tile]                    # (bn,d)\n",
    "            muq_i  = mu_quad[i0:i0 + Nn_tile]                 # (bn,)\n",
    "            dX_i   = dX[i0:i0 + Nn_tile]                      # (bn,d)\n",
    "\n",
    "            cross = muAh_i @ X.t()                            # (bn,mb)\n",
    "            expo  = log_norm_const - 0.5 * (muq_i[:, None] + X_quad[None, :] - 2.0 * cross)\n",
    "\n",
    "            if stable:\n",
    "                w = torch.exp(expo - lse_max[None, :])        # (bn,mb)\n",
    "            else:\n",
    "                w = torch.exp(expo)\n",
    "\n",
    "            denom_tile += w.sum(dim=0, dtype=torch.float64) / (N * n)\n",
    "            numer_tile += (w.t() @ dX_i).to(torch.float64) * ((t1 - t0) / N)\n",
    "\n",
    "        if stable:\n",
    "            scale = torch.exp(lse_max.to(torch.float64))\n",
    "            denom_tile *= scale\n",
    "            numer_tile *= scale[:, None]\n",
    "\n",
    "        denom[m0:m0 + X.size(0)] += denom_tile\n",
    "        numer[m0:m0 + X.size(0)] += numer_tile\n",
    "\n",
    "    est = (numer / denom[:, None]).to(torch.float32)          # (M,d)\n",
    "\n",
    "    if truncate:\n",
    "        m = denom.min()\n",
    "        est[denom <= (m / 2.0)] = 0\n",
    "\n",
    "    return est\n",
    "def prepare_for_nadaraya(config):\n",
    "    deltaT = config.deltaT\n",
    "    t1 = deltaT * config.ts_length\n",
    "    H = config.hurst\n",
    "    is_path_observations = np.load(config.data_path, allow_pickle=True)[:num_paths, :, :]\n",
    "    is_path_observations = np.concatenate(\n",
    "        [np.repeat(np.array(config.initState).reshape((1, 1, config.ndims)), is_path_observations.shape[0], axis=0),\n",
    "         is_path_observations], axis=1)\n",
    "    assert is_path_observations.shape == (num_paths, config.ts_length + 1, config.ndims)\n",
    "    path_observations = is_path_observations\n",
    "    t0 = deltaT\n",
    "    prevPath_observations = path_observations[:, 1:-1, :]\n",
    "    path_incs = np.diff(path_observations, axis=1)[:, 1:, :]\n",
    "    assert (prevPath_observations.shape == path_incs.shape)\n",
    "    assert (path_incs.shape[1] == config.ts_length - 1)\n",
    "    assert (path_observations.shape[1] == prevPath_observations.shape[1] + 2)\n",
    "    assert (prevPath_observations.shape[1] * deltaT == (t1 - t0))\n",
    "    return is_path_observations\n",
    "\n",
    "def run_nadaraya_single_bw(config, is_path_observations, states, M_tile):\n",
    "    bw = np.logspace(-3.55, -0.05, 30)[[5]]\n",
    "    inv_H = np.diag(np.power(bw, -2))\n",
    "    norm_const = 1 / np.sqrt((2. * np.pi) ** config.ndims * (1. / np.linalg.det(inv_H)))\n",
    "    Nn_tile = 512000\n",
    "    stable = True\n",
    "    Xs = torch.as_tensor(states, dtype=torch.float32, device=device_id).contiguous()\n",
    "    is_ss_path_observations = is_path_observations\n",
    "    is_prevPath_observations = is_ss_path_observations[:, 1:-1]\n",
    "    is_path_incs = np.diff(is_ss_path_observations, axis=1)[:, 1:]\n",
    "    is_prevPath_observations = torch.as_tensor(is_prevPath_observations, dtype=torch.float32,\n",
    "                                               device=device_id).contiguous()\n",
    "    is_path_incs = torch.as_tensor(is_path_incs, dtype=torch.float32, device=device_id).contiguous()\n",
    "    # inv_H: prefer diagonal vector if possible\n",
    "    inv_H_np = np.asarray(inv_H)\n",
    "    if inv_H_np.ndim == 2 and np.allclose(inv_H_np, np.diag(np.diag(inv_H_np))):\n",
    "        inv_H_vec = np.diag(inv_H_np).astype(np.float32)\n",
    "        inv_H = torch.as_tensor(inv_H_vec, device=device_id)\n",
    "    else:\n",
    "        inv_H = torch.as_tensor(inv_H_np.astype(np.float32), device=device_id)\n",
    "\n",
    "    unif_is_drift_hats = IID_NW_multivar_estimator_gpu(\n",
    "        is_prevPath_observations, is_path_incs, inv_H, float(norm_const),\n",
    "        Xs, float(config.t1), float(config.t0),\n",
    "        truncate=True, M_tile=M_tile, Nn_tile=Nn_tile, stable=stable\n",
    "    ).cpu().numpy()\n",
    "    return unif_is_drift_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 8DLnz\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 69\u001B[0m\n\u001B[1;32m     67\u001B[0m mse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mpower(true_drift \u001B[38;5;241m-\u001B[39m all_nad_drift_ests,\u001B[38;5;241m2\u001B[39m), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     68\u001B[0m nad_eval[ts_type] \u001B[38;5;241m=\u001B[39m mse\n\u001B[0;32m---> 69\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m     71\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m~/GitHubRepos/FractionalBrownianMotion/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:686\u001B[0m, in \u001B[0;36msynchronize\u001B[0;34m(device)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msynchronize\u001B[39m(device: _device_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    679\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Waits for all kernels in all streams on a CUDA device to complete.\u001B[39;00m\n\u001B[1;32m    680\u001B[0m \n\u001B[1;32m    681\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    684\u001B[0m \u001B[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001B[39;00m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 686\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    687\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[1;32m    688\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_synchronize()\n",
      "File \u001B[0;32m~/GitHubRepos/FractionalBrownianMotion/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import gc, time\n",
    "score_eval = {t: np.inf for t in [\"8DLnz\", \"12DLnz\", \"20DLnz\", \"40DLnz\"]}\n",
    "nad_eval = {t: np.inf for t in [\"8DLnz\", \"12DLnz\", \"20DLnz\", \"40DLnz\"]}\n",
    "for config in [lnz_8d_config, lnz_12d_config, lnz_20d_config, lnz_40d_config]:\n",
    "    assert config.feat_thresh == 1.\n",
    "    assert config.forcing_const == 1.25\n",
    "    Xshape = config.ts_length\n",
    "    root_score_dir = root_dir\n",
    "    label = \"$\\mu_{5}$\"\n",
    "    if \"8DLnz\" in config.data_path:\n",
    "        root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/8DLnzChaosLessData/\"\n",
    "        ts_type = \"8DLnz\"\n",
    "    elif \"12DLnz\" in config.data_path:\n",
    "        root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/12DLnzChaosLessData/\"\n",
    "        ts_type = \"12DLnz\"\n",
    "    elif \"20DLnz\" in config.data_path:\n",
    "        root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/20DLnzChaosLessData/\"\n",
    "        ts_type = \"20DLnz\"\n",
    "    elif \"40DLnz\" in config.data_path:\n",
    "        root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/40DLnzChaosLessData/\"\n",
    "        ts_type = \"40DLnz\"\n",
    "    print(f\"Starting {ts_type}\\n\")\n",
    "    model_dir = \"/\".join(config.scoreNet_trained_path.split(\"/\")[:-1]) + \"/\"\n",
    "    entered = False\n",
    "    best_epoch = get_best_epoch(config=config,type=\"EE\")\n",
    "    for file in os.listdir(model_dir):\n",
    "        if config.scoreNet_trained_path in os.path.join(model_dir, file) and (\"EE\" in file and \"Trk\" not in file) and str(best_epoch) in file:\n",
    "            good = ConditionalMarkovianTSPostMeanScoreMatching(\n",
    "        *config.model_parameters)\n",
    "            entered = True\n",
    "            good.load_state_dict(torch.load(os.path.join(model_dir, file)))\n",
    "    assert entered\n",
    "    good = good.to(device_id)\n",
    "    good.eval()\n",
    "    all_true_paths, all_global_paths = generate_synthetic_paths(config=config, device_id=device_id, good=good)\n",
    "    all_true_paths = all_true_paths.reshape(-1, config.ts_length+1, config.ts_dims)\n",
    "    all_global_paths = all_global_paths.reshape(-1, config.ts_length+1, config.ts_dims)\n",
    "    all_true_states = all_true_paths[:, 1:,:].reshape(-1, config.ts_dims)\n",
    "    all_global_states = all_global_paths[:, 1:,:].reshape(-1, config.ts_dims)\n",
    "    true_drift = true_drifts(state=all_true_states, device_id=device_id,config=config).cpu().numpy()[:, 0,:]\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(5)\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    time.sleep(5)\n",
    "    all_score_drift_ests = np.zeros_like(true_drift)\n",
    "    all_nad_drift_ests = np.zeros_like(true_drift)\n",
    "    block_size = 1024\n",
    "    is_obs = prepare_for_nadaraya(config=config)\n",
    "    for k in tqdm(range(0, all_global_states.shape[0], block_size)):\n",
    "        curr_states = torch.tensor(all_global_states[k:k+block_size, :], device=device_id, dtype=torch.float32)\n",
    "        drift_ests = experiment_MLP_DDims_drifts(config=config, Xs=curr_states, good=good, onlyGauss=False)\n",
    "        drift_ests= drift_ests[:, -1, :, :].reshape(drift_ests.shape[0],drift_ests.shape[2],drift_ests.shape[\n",
    "                                                                                                -1] * 1).mean(axis=1)\n",
    "        all_score_drift_ests[k:k+block_size,:] = drift_ests\n",
    "        nad_drift_est = run_nadaraya_single_bw(config=config, is_path_observations=is_obs, states=curr_states, M_tile=block_size)\n",
    "        all_nad_drift_ests[k:k+block_size,:] = nad_drift_est\n",
    "        del curr_states\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    mse = np.mean(np.sum(np.power(true_drift - all_score_drift_ests,2), axis=-1))\n",
    "    score_eval[ts_type] = mse\n",
    "    mse = np.mean(np.sum(np.power(true_drift - all_nad_drift_ests,2), axis=-1))\n",
    "    nad_eval[ts_type] = mse\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "save_path = (project_config.ROOT_DIR + f\"experiments/results/DLnzChaos_{config.ndims}_NewDriftEvalExp_MSEs_{num_paths}NPaths\").replace(\n",
    "            \".\", \"\")\n",
    "pd.DataFrame.from_dict(score_eval, orient=\"index\", columns=[\"mse\"]).to_parquet(save_path + \"_score_MSE.parquet\")\n",
    "pd.DataFrame.from_dict(nad_eval, orient=\"index\", columns=[\"mse\"]).to_parquet(save_path + \"_nad_MSE.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
