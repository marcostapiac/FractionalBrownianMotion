{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from configs import project_config\n",
    "from configs.RecursiveVPSDE.Markovian_12DLorenz.recursive_Markovian_PostMeanScore_12DLorenz_Stable_T256_H05_tl_110data_StbleTgt import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rmse_ignore_nans(y_true, y_pred):\n",
    "    assert (y_true.shape[0] == y_pred.shape[0])\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)  # Ignore NaNs in both arrays\n",
    "    return (np.mean((y_true[mask] - y_pred[mask]) ** 2))\n",
    "\n",
    "def plot_ewma_losses(epochs, losses):\n",
    "    start_idx = 00\n",
    "    def compute_ema(loss_tensor, beta):\n",
    "        ema_values = np.zeros_like(loss_tensor)  # Initialize EMA tensor\n",
    "        ema_values[0] = loss_tensor[0]  # First value stays the same\n",
    "\n",
    "        for i in range(1, len(loss_tensor)):\n",
    "            ema_values[i] = beta * ema_values[i - 1] + (1 - beta) * loss_tensor[i]\n",
    "\n",
    "        return ema_values\n",
    "    # Define EMA decay rates\n",
    "    beta_short = 0.9   # Short-term trend (reacts quickly)\n",
    "    beta_long = 0.99   # Long-term trend (smoother)\n",
    "\n",
    "    # Compute EMAs\n",
    "    short_term_ema = compute_ema(losses, beta_short)\n",
    "    long_term_ema = compute_ema(losses, beta_long)\n",
    "    plt.scatter(epochs[start_idx:], (short_term_ema[start_idx:]),s=2, label=\"Short Term Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.scatter(epochs[start_idx:], (long_term_ema[start_idx:]),s=2, label=\"Long Term Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "deltaT = config.deltaT\n",
    "root_dir =\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/\"\n",
    "print(config.loss_factor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss\", 'rb') as f:\n",
    "        losses = (np.array(pickle.load(f)).astype(float))\n",
    "Nepochs_losses = np.arange(losses.shape[0])\n",
    "plt.scatter(Nepochs_losses,  losses, s=10)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "Nepochs_losses[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_ewma_losses(epochs=Nepochs_losses, losses=losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss_LR\", 'rb') as f:\n",
    "        LRs = (np.array(pickle.load(f)).astype(float))\n",
    "start = 0#935\n",
    "end = -1#935+152\n",
    "plt.scatter(Nepochs_losses[start:end],  LRs[start:end], s=10, label=\"Learning Rate\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(end-start+1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_drift_files(config, root_dir):\n",
    "    ts_type = f\"ST_{config.feat_thresh:.3f}FTh_{config.ndims}DLnz\".replace(\".\", \"\")\n",
    "    include =  (f\"_{1}NDT_{config.loss_factor}LFac_BetaMax{config.beta_max:.1e}_{round(config.forcing_const,3)}FConst\").replace(\n",
    "            \".\", \"\")\n",
    "    root_score_dir = root_dir + f\"ExperimentResults/TSPM_Markovian/12DLnz/\"\n",
    "    driftoostrack_true_files = []\n",
    "    driftoostrack_local_files = []\n",
    "    for file in os.listdir(root_score_dir):\n",
    "        if \"MLP_\" in file and ts_type in file and include in file:\n",
    "            if \"DriftTrack\" in file and \"true\" in file:\n",
    "                driftoostrack_true_files.append(root_score_dir+file)\n",
    "            elif \"DriftTrack\" in file and \"global\" in file:\n",
    "                driftoostrack_local_files.append(root_score_dir+file)\n",
    "    assert len(driftoostrack_true_files)>0, \"No oos drift track files found\"\n",
    "    assert len(driftoostrack_local_files)>0, \"No oos drift track files found\"\n",
    "    assert(len(driftoostrack_true_files) == len(driftoostrack_true_files))\n",
    "    def extract_bw_drift_track_number(s):\n",
    "        match = s.split(\"Nep_\")[0].split(\"_\")[-1]\n",
    "        return int(match)\n",
    "    driftoostrack_true_files = sorted(driftoostrack_true_files, key=extract_bw_drift_track_number)\n",
    "    driftoostrack_local_files = sorted(driftoostrack_local_files, key=extract_bw_drift_track_number)\n",
    "    Nepochs_track = [extract_bw_drift_track_number(f) for f in driftoostrack_true_files]\n",
    "    return driftoostrack_true_files, driftoostrack_local_files, Nepochs_track"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drift_true_files, drift_local_files, Nepochs_track = get_drift_files(config,root_dir=root_dir)\n",
    "len(Nepochs_track)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drift_track_rmses = {}\n",
    "epochs_iter = Nepochs_track\n",
    "Nepochs_keep = []\n",
    "for Nepoch, true, local in zip(*[epochs_iter, drift_true_files, drift_local_files]):\n",
    "    try:\n",
    "        all_true_states = np.load(true)\n",
    "        all_local_states= np.load(local)\n",
    "        time_steps = np.linspace(config.t0,config.deltaT*all_true_states.shape[2],all_true_states.shape[2])\n",
    "        all_global_errors = np.sum(np.power(all_true_states- all_local_states,2), axis=-1)\n",
    "        all_global_errors=all_global_errors.reshape(-1, all_global_errors.shape[-1])            # (K, N, T)\n",
    "        total_local_errors = np.sqrt(np.mean(all_global_errors, axis=(0)))/np.sqrt(time_steps)\n",
    "        all_paths_err = np.sqrt(all_global_errors)/np.sqrt(time_steps)\n",
    "        total_local_errors[total_local_errors==np.inf] = 0.\n",
    "        all_paths_err[all_paths_err==np.inf] = 0.\n",
    "        drift_track_rmses.update({Nepoch:total_local_errors[-1]})\n",
    "        Nepochs_keep.append(Nepoch)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(e)\n",
    "        continue\n",
    "Nepochs_track = Nepochs_keep\n",
    "drift_track_rmses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(config.scoreNet_trained_path.replace(\"/trained_models/\", \"/training_losses/\") + \"_loss\", 'rb') as f:\n",
    "        losses = np.array(pickle.load(f)).astype(float)\n",
    "Nepochs_losses = np.arange(losses.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "common_epochs = np.intersect1d(Nepochs_losses, Nepochs_track)\n",
    "common_epochs = np.intersect1d(common_epochs, np.arange(1, 3500))\n",
    "start_idx = 0\n",
    "common_epochs = common_epochs[start_idx:]\n",
    "losses_idx = [np.argwhere(c == Nepochs_losses)[0,0] for c in common_epochs]\n",
    "track_idx = [np.argwhere(c == np.array(list(drift_track_rmses.keys())))[0,0] for c in common_epochs]\n",
    "red_losses = losses[losses_idx]\n",
    "track_rmses = np.array(list(drift_track_rmses.values()))[track_idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(common_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "toSave = True\n",
    "fig, ax = plt.subplots(figsize=(14,9))\n",
    "sc1 = ax.scatter(common_epochs, red_losses, s=10, label=\"Training Loss\")\n",
    "ax2 = ax.twinx()\n",
    "sc2 = ax2.scatter(common_epochs, track_rmses, s=20, color=\"red\",label=\"Tracking MSE\")\n",
    "ax.set_xlabel(\"Training Epochs\", fontsize=38)\n",
    "ax.tick_params(axis=\"both\",labelsize=24)\n",
    "ax2.tick_params(axis=\"both\",labelsize=24)\n",
    "ax.set_title(r\"Losses for $\\mu_{6}$\", fontsize=40)\n",
    "\n",
    "#ax.set_yscale(\"log\")\n",
    "#ax2.set_yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "handles = [sc1, sc2]\n",
    "labels = [h.get_label() for h in handles]\n",
    "# Add a single legend on ax1\n",
    "ax.legend(handles, labels, fontsize=24)\n",
    "if toSave:\n",
    "    plt.savefig((f\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/DiffusionModelPresentationImages/TSPM_Markovian/12DLnz/TSPM_MLP_PM_ST_{config.feat_thresh:.3f}FTh_{config.ndims}DLnz_{config.forcing_const}FConst_LossesTrack_{config.loss_factor}LFac\").replace(\".\",\"\")+\".png\",  bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "curr_rmse = np.inf\n",
    "toSave = True\n",
    "for f_idx in range(len(common_epochs)):\n",
    "    Nepoch = common_epochs[f_idx]\n",
    "    ff_idx = np.argwhere(Nepoch == np.array(epochs_iter))[0,0]\n",
    "    true = drift_true_files[ff_idx]\n",
    "    local = drift_local_files[ff_idx]\n",
    "    try:\n",
    "        if Nepoch < 1: raise FileNotFoundError\n",
    "        num_diff_times = 1\n",
    "        all_true_states = np.load(true)\n",
    "        all_local_states= np.load(local)\n",
    "        print(all_local_states.shape)\n",
    "        time_steps = np.linspace(config.t0,config.deltaT*all_true_states.shape[2],all_true_states.shape[2])\n",
    "        all_global_errors = np.sum(np.power(all_true_states- all_local_states,2), axis=-1)\n",
    "        all_global_errors=all_global_errors.reshape(-1, all_global_errors.shape[-1])            # (K, N, T)\n",
    "        total_local_errors = np.sqrt(np.mean(all_global_errors, axis=(0)))/np.sqrt(time_steps)\n",
    "        all_paths_err = np.sqrt(all_global_errors)/np.sqrt(time_steps)\n",
    "        total_local_errors[total_local_errors==np.inf] = 0.\n",
    "        all_paths_err[all_paths_err==np.inf] = 0.\n",
    "        total_local_errors_minq, total_local_errors_maxq  = np.quantile(all_paths_err, q=[0.005, 0.995], axis=0) # (T,)<\n",
    "        if total_local_errors[-1] < curr_rmse:\n",
    "            curr_rmse = total_local_errors[-1]\n",
    "            fig, ax = plt.subplots(figsize=(14,9))\n",
    "            plt.grid(True)\n",
    "            ax.scatter(time_steps, total_local_errors)\n",
    "            plt.fill_between(time_steps,y1=total_local_errors_minq, y2=total_local_errors_maxq, color=\"blue\", alpha=0.4)\n",
    "            ax.set_title(rf\"Pathwise RMSE for Score Estimator for $\\mu_6$\",fontsize=40)\n",
    "            ax.set_ylabel(\"RMSE\", fontsize=38)\n",
    "            ax.set_xlabel(\"Time Axis\", fontsize=38)\n",
    "            ax.tick_params(labelsize=38)\n",
    "            plt.tight_layout()\n",
    "            if toSave:\n",
    "                plt.savefig((f\"/Users/marcos/Library/CloudStorage/OneDrive-ImperialCollegeLondon/StatML_CDT/Year2/DiffusionModels/DiffusionModelPresentationImages/TSPM_Markovian/12DLnz/TSPM_MLP_PM_ST_{config.feat_thresh:.3f}FTh_{config.ndims}DLnz_DriftTrack_{Nepoch}Nep_{config.forcing_const}FConst_{round(total_local_errors_minq[-1], 8)}_MinIQR_{round(total_local_errors[-1], 8)}MeanIQR_{round(total_local_errors_maxq[-1], 8)}_MaxIQR\").replace(\".\", \"\")+\".png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            print(f\"Final time cumulative MSE local-time error {total_local_errors[-1]} with final IQR {(total_local_errors_minq[-1], total_local_errors_maxq[-1])} at Nepoch {Nepoch} for {config.forcing_const}FConst\\n\")\n",
    "    except FileNotFoundError as e:\n",
    "        continue\n",
    "# 0.021385787 final time error for 12DLnz at epoch 94 before changing final_tau = init_tau = 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.generative_modelling.models.TimeDependentScoreNetworks.ClassConditionalMarkovianTSPostMeanScoreMatching import \\\n",
    "    ConditionalMarkovianTSPostMeanScoreMatching\n",
    "import torch\n",
    "scoreModel1 = ConditionalMarkovianTSPostMeanScoreMatching(\n",
    "        *config.model_parameters)\n",
    "rootdir = \"/Users/marcos/GitHubRepos/FractionalBrownianMotion/src/generative_modelling/trained_models/\"\n",
    "modelname = config.scoreNet_trained_path.replace(rootdir, \"\")\n",
    "for file in os.listdir(rootdir):\n",
    "    if modelname in file:\n",
    "        scoreModel1.load_state_dict(torch.load(rootdir+file))\n",
    "        print(file)\n",
    "tau1 =  max(scoreModel1.mlp_state_mapper.hybrid.final_tau,\n",
    "                          scoreModel1.mlp_state_mapper.hybrid.init_tau * (0.9 ** (94 // 20)))\n",
    "scoreModel1.eval()\n",
    "scoreModel2 = ConditionalMarkovianTSPostMeanScoreMatching(*config.model_parameters)\n",
    "rootdir = \"/Users/marcos/GitHubRepos/FractionalBrownianMotion/src/generative_modelling/snapshots/\"\n",
    "for file in os.listdir(rootdir):\n",
    "    if modelname in file:\n",
    "        scoreModel2.load_state_dict(torch.load(rootdir+file, map_location=torch.device(\"cpu\"))[\"MODEL_STATE\"])\n",
    "scoreModel2.eval()\n",
    "tau2 =  max(scoreModel2.mlp_state_mapper.hybrid.final_tau,\n",
    "                          scoreModel2.mlp_state_mapper.hybrid.init_tau * (0.9 ** (Nepochs_losses[-1] // 20)))\n",
    "print(tau1, tau2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Loop through named parameters\n",
    "for (name_5, param_5), (name_20, param_20) in zip(scoreModel1.named_parameters(), scoreModel2.named_parameters()):\n",
    "    assert name_5 == name_20  # Sanity check: same parameter names\n",
    "\n",
    "    # Compute L2 norm of difference\n",
    "    diff = torch.norm(param_5.data - param_20.data).item()\n",
    "    print(f\"{name_5}: L2 norm difference = {diff:.6f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.mean(torch.exp(scoreModel2.mlp_state_mapper.hybrid.log_scale)), torch.mean(torch.sigmoid(scoreModel2.mlp_state_mapper.hybrid.gate_logits / 0.5)), scoreModel2.mlp_state_mapper.hybrid.tau"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Nepoch_good = 170000\n",
    "Nepoch_bad = 300\n",
    "good_idx = np.argwhere(common_epochs == Nepoch_good)[0,0]\n",
    "bad_idx = np.argwhere(common_epochs == Nepoch_bad)[0,0]\n",
    "good_true = np.load(drift_true_files[good_idx])[0, :, :, :]\n",
    "good_local = np.load(drift_local_files[good_idx])[0, :,:, :]\n",
    "bad_true = np.load(drift_true_files[bad_idx])[0, :, :, :]\n",
    "bad_local = np.load(drift_local_files[bad_idx])[0, :, :,:]\n",
    "print(bad_local.shape, bad_true.shape)\n",
    "B, T, D = bad_local.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for dim in range(config.ndims):\n",
    "    dim_idx = dim + 1\n",
    "    time_ax = np.linspace(config.t0,config.deltaT*T,T)\n",
    "    # Print different \"true sample paths\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for _ in range(B):\n",
    "        axes[0].scatter(time_ax, good_true[_, :, dim]-good_local[_, :, dim], color=\"red\", s=1)\n",
    "        axes[1].scatter(time_ax, bad_true[_, :, dim]-bad_local[_, :, dim], color=\"blue\", s=1)\n",
    "    plt.suptitle(f\"True vs Score-Based Path Difference for Dimension {dim_idx}\\n\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Path\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
